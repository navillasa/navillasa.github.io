<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-19T06:32:12-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Natalie Villasana</title><subtitle>DevOps Engineer | navillasa.dev</subtitle><author><name>Natalie Villasana</name><email>navillasa.dev@gmail.com</email></author><entry><title type="html">Building Production-Grade Infrastructure for a Simple App</title><link href="http://localhost:4000/blog/tv-hub-project-overview/" rel="alternate" type="text/html" title="Building Production-Grade Infrastructure for a Simple App" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/tv-hub-project-overview</id><content type="html" xml:base="http://localhost:4000/blog/tv-hub-project-overview/">## The Paradox of Portfolio Projects

I faced a classic engineering dilemma: **build something complex that needs sophisticated infrastructure, or build something simple and add sophisticated infrastructure anyway?**

After cycling through ideas (distributed microservices, ML pipelines, IoT dashboards) I kept returning to one idea: a TV show aggregator. Because it was an app with some appeal and use beyond demonstrating its own tech stack. The app itself is straightforward: fetch trending shows from TMDB and TVmaze APIs, rank by popularity, display in a clean interface.

**The twist?** I deployed it with enterprise-grade DevOps practices that would typically serve a team of a 50+ engineers. Or at least a dozen.

## Strategic Design Decisions

### **Simple App, Complex Infrastructure**

I specifically chose a simple application to isolate and focus on infrastructure complexity. Here's why this approach worked:

**Cognitive Load Management**: Managing ArgoCD, Vault integration, and Prometheus monitoring is challenging enough without debugging distributed system race conditions simultaneously.

**Clear Separation of Concerns**: When something breaks, I can immediately identify whether it's an application bug (less likely, given the simplicity) or an infrastructure configuration issue (very likely, given the learning curve).

**Real-World Simulation**: Most production systems start simple and grow complex. This project demonstrates the infrastructure foundation that scales.

## Technical Architecture

The application architecture is intentionally minimal:

```
React Frontend ←→ Node.js API ←→ PostgreSQL ←→ External APIs
```

The **infrastructure architecture** tells a different story:

```
GitHub → Actions → GCR → ArgoCD → GKE Autopilot
    ↓                        ↓
Terraform → GCP Resources   Vault → External Secrets → K8s Secrets
    ↓                        ↓
Monitoring ← Prometheus ← Applications → Logs
```

### **Production-Grade Components**

**GitOps with ArgoCD**: Declarative deployments with dev/staging/prod environment promotion. Changes flow through Git, not `kubectl apply`.

**Infrastructure as Code**: Terraform manages GKE clusters, static IPs, DNS records, and IAM policies.

**Secrets Management**: HashiCorp Vault with External Secrets Operator. API keys and database credentials never touch Git or container images.

**Multi-Environment Overlays**: Kustomize overlays handle environment-specific configurations. Production can use different resource limits, replica counts, and ingress rules than development.

**Observability Stack**: Prometheus metrics collection with Grafana dashboards. Custom application metrics track API response times and cache hit rates.

## What I Learned

### **1. Mental Model Clarity**

Complex infrastructure demands clear mental models. When ArgoCD shows &quot;OutOfSync&quot; status, I need to immediately understand:
- Which Git commit should be deployed?
- What Kustomize transformations apply?
- How do external secrets affect pod startup?

**Practical Impact**: Reduced debugging time from hours to minutes by maintaining clear architectural diagrams and understanding component dependencies.

### **2. Architectural Trade-offs**

Every decision has implications. For example: [Vault as ArgoCD Application]({{ '/blog/vault-gitops/' | relative_url }}) explores why I kept secrets management outside GitOps workflows.

**Key Trade-off**: GitOps consistency vs. bootstrap complexity. Pure GitOps purists would manage Vault through ArgoCD, but circular dependencies make this operationally complex.

**Decision Framework**: Evaluate based on operational simplicity, recovery scenarios, and production patterns, not theoretical purity.

### **3. Systematic Debugging**

Production systems fail in creative ways. My approach: [The K8s Doctor]({{ '/blog/debugging-502-errors/' | relative_url }}) demonstrates systematic troubleshooting methodology.

**Core Principle**: Most failures are configuration mismatches, not infrastructure failures. Port conflicts, service discovery issues, and protocol mismatches accounted for the vast majority of my deployment problems.

## Why This Approach Works

### **For Learning**
Starting with a simple application let me focus on infrastructure concepts without application complexity interference. I could experiment with GitOps workflows, monitoring configurations, and security policies without worrying about business logic bugs. For the most part :)

### **For Demonstrating**
**Systems Thinking**: Understanding how components interact across multiple layers (application, orchestration, infrastructure, security).

**Production Readiness**: Moving beyond &quot;it works on my machine&quot; to &quot;it works reliably in production with proper monitoring, security, and deployment practices.&quot;

**Technology Integration**: Connecting multiple tools (ArgoCD, Vault, Prometheus, Terraform) into cohesive workflows rather than using them in isolation.

**Decision Making**: Choosing appropriate architectural patterns based on trade-offs. A lot more fun, and difficult, than following tutorials.

## Current Status
[Live application](http://tv-hub.navillasa.dev) with full GitOps deployment, multi-environment promotion, and comprehensive monitoring.

**Next Phases**: Cost optimization dashboard, Redis caching layer, and advanced alerting rules.

## The Meta-Learning

This project demonstrates that I can build production-ready infrastructure, make informed architectural decisions, and systematically debug complex systems.

More than learning how to use individual tools, I learned how they integrate, _when_ to use them, and what trade-offs each decision creates.

Building infrastructure for a simple application taught me more about production DevOps than building a complex application with simple deployment ever could.</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="architecture" /><category term="kubernetes" /><category term="argocd" /><category term="terraform" /><category term="gitops" /><category term="gke" /><category term="portfolio" /><summary type="html">The Paradox of Portfolio Projects</summary></entry><entry><title type="html">The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s</title><link href="http://localhost:4000/blog/debugging-502-errors/" rel="alternate" type="text/html" title="The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/debugging-502-errors</id><content type="html" xml:base="http://localhost:4000/blog/debugging-502-errors/">&lt;h2 id=&quot;the-problem-pattern&quot;&gt;The Problem Pattern&lt;/h2&gt;

&lt;p&gt;Throughout this project, I’ve hit the same type of issue repeatedly:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt; looking for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dev-backend-service&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prod-backend-service&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ArgoCD ingress&lt;/strong&gt; using port 443 instead of port 80&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vault&lt;/strong&gt; connection errors due to service discovery issues&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;External Secrets&lt;/strong&gt; pointing to wrong Vault paths&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The common thread&lt;/strong&gt;: 502 Bad Gateway errors caused by communication mismatches.&lt;/p&gt;

&lt;h2 id=&quot;the-502-error-what-it-really-means&quot;&gt;The 502 Error: What It Really Means&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;502 Bad Gateway = &quot;I can reach something, but it's not responding correctly&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;This is different from:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;404&lt;/strong&gt;: “I can’t find anything at this path”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;503&lt;/strong&gt;: “Service is temporarily unavailable”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Connection timeout&lt;/strong&gt;: “I can’t reach anything at all”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;502 specifically means&lt;/strong&gt;: The proxy/load balancer reached a backend, but the backend response was invalid.&lt;/p&gt;

&lt;h2 id=&quot;the-systematic-debugging-approach&quot;&gt;The Systematic Debugging Approach&lt;/h2&gt;

&lt;h3 id=&quot;step-1-identify-the-communication-chain&quot;&gt;Step 1: Identify the Communication Chain&lt;/h3&gt;

&lt;p&gt;Every 502 has a path like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Client → Load Balancer → Service → Pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Find where it breaks by checking each link.&lt;/p&gt;

&lt;h3 id=&quot;step-2-check-backend-health-in-load-balancer&quot;&gt;Step 2: Check Backend Health in Load Balancer&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Get detailed ingress status&lt;/span&gt;
kubectl describe ingress &amp;lt;ingress-name&amp;gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; &amp;lt;namespace&amp;gt;

&lt;span class=&quot;c&quot;&gt;# Look for backend health status&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Example output:&lt;/span&gt;
ingress.kubernetes.io/backends: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;k8s1-backend-service-443&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;UNHEALTHY&quot;&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# ← The smoking gun!&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Key indicators:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNHEALTHY&lt;/code&gt; = Backend isn’t responding on expected port/protocol&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HEALTHY&lt;/code&gt; = Backend is fine, look elsewhere&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pro-tips-for-faster-debugging&quot;&gt;Pro Tips for Faster Debugging&lt;/h2&gt;

&lt;h3 id=&quot;1-use-port-forward-for-direct-testing&quot;&gt;1. &lt;strong&gt;Use Port-Forward for Direct Testing&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Bypass ingress/service and test pod directly&lt;/span&gt;
kubectl port-forward pod/&amp;lt;pod-name&amp;gt; 8080:8080
curl http://localhost:8080/health
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-check-multiple-namespaces&quot;&gt;2. &lt;strong&gt;Check Multiple Namespaces&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Service in wrong namespace is super common&lt;/span&gt;
kubectl get services &lt;span class=&quot;nt&quot;&gt;--all-namespaces&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &amp;lt;service-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key insight: Most 502 errors are configuration mismatches, not infrastructure failures. A systematic approach beats random troubleshooting every time.&lt;/p&gt;</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="debugging" /><category term="devops" /><category term="kubernetes" /><category term="502-errors" /><category term="troubleshooting" /><category term="nginx" /><category term="ingress" /><summary type="html">The Problem Pattern</summary></entry><entry><title type="html">Why I Didn’t Make Vault an ArgoCD Application</title><link href="http://localhost:4000/blog/vault-gitops/" rel="alternate" type="text/html" title="Why I Didn’t Make Vault an ArgoCD Application" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/vault-gitops</id><content type="html" xml:base="http://localhost:4000/blog/vault-gitops/">## The Question

During the GitOps implementation, I considered whether HashiCorp Vault should be managed as an ArgoCD application alongside the other workloads. This post explains why I chose to keep Vault outside of the GitOps workflow.

## The Temptation

At first glance, managing Vault through ArgoCD seems _pretty_ cool:

- **Consistency**: Everything else is managed by ArgoCD
- **Version Control**: Vault configuration tracked in Git
- **Declarative**: Infrastructure as Code principles
- **Automation**: Automated deployments and updates

## The Bootstrap Problem

The fundamental issue is a **circular dependency**:

```
┌─────────────┐    needs    ┌──────────────────┐    needs    ┌───────────┐
│   ArgoCD    │ ──────────► │ External Secrets │ ──────────► │   Vault   │
│             │             │    Operator      │             │           │
└─────────────┘             └──────────────────┘             └───────────┘
       ▲                                                            │
       │                    manages (if GitOps)                     │
       └────────────────────────────────────────────────────────────┘
```

**The problem**: If ArgoCD manages Vault, then ArgoCD depends on Vault (for secrets) which depends on ArgoCD (for deployment). This creates an unresolvable bootstrap dependency.

## Infrastructure Layers

I solved this by establishing clear **infrastructure layers**:

### Layer 0: Foundation Infrastructure
- **GKE Cluster** (Terraform)
- **Static IPs** (Terraform)
- **Vault** (kubectl apply)
- **External Secrets Operator** (Helm)

### Layer 1: GitOps Platform
- **ArgoCD** (kubectl apply)
- **ClusterSecretStore** (ArgoCD)

### Layer 2: Applications
- **tv-dashboard-dev** (ArgoCD)
- **tv-dashboard-prod** (ArgoCD)
- **monitoring-stack** (ArgoCD)

## Alternative Approaches Considered

### 1. Manual Sync Only
```yaml
# vault-app.yaml
spec:
  syncPolicy:
    automated: null  # No auto-sync to avoid bootstrap issues
```

**Pros**: Vault config in Git  
**Cons**: Manual intervention required, defeats GitOps automation

### 2. App-of-Apps Pattern
```yaml
# bootstrap-app.yaml - deployed manually
# Manages vault-app.yaml and argocd-apps.yaml
```

**Pros**: Complete GitOps coverage  
**Cons**: Complex bootstrap sequence, harder to debug

### 3. External Vault
Use a managed secret service (Google Secret Manager, AWS Secrets Manager)

**Pros**: No bootstrap problem, less operational overhead  
**Cons**: Vendor lock-in, less learning value for this project

## The Decision: Keep Vault Outside GitOps

**Reasons**:

1. **Operational Simplicity**: Vault is foundational infrastructure that should be stable and simple to manage
2. **Recovery Scenarios**: If ArgoCD fails, we can still access Vault to recover secrets
3. **Bootstrap Clarity**: Clear separation between foundation and application layers
4. **Production Patterns**: Many organizations treat secret management as Layer 0 infrastructure

## Production Considerations

In production environments, you might see:

### At Large Organizations
- **Dedicated Vault clusters** managed by platform teams
- **Vault-as-a-Service** provided to application teams
- **Manual deployment** with infrastructure automation (Terraform)

### At Cloud-Native Shops
- **Managed secret services** (AWS Secrets Manager, etc.)
- **External Secrets Operator** connecting to cloud providers
- **No self-hosted Vault** at all

### In GitOps Purist Repos
- **Everything in Git** including Vault
- **Complex bootstrap procedures** with operator dependencies
- **Acceptance of operational complexity** for consistency

## Key Takeaway

**Not everything needs to be in GitOps.** The right architectural boundary depends on:

- **Operational complexity** vs **consistency benefits**
- **Bootstrap dependencies** and **recovery scenarios**  
- **Team structure** and **operational expertise**
- **Compliance requirements** and **audit trails**

For my TV Dashboard project, keeping Vault as foundational infrastructure provided the right balance of simplicity and functionality while still demonstrating modern secret management practices.

## What I Learned

1. **Architectural boundaries matter** - not every tool fits every pattern
2. **Bootstrap dependencies** are real constraints in system design
3. **Operational simplicity** often trumps theoretical consistency
4. **Production patterns** should inform learning project decisions

The goal isn't to GitOps-ify everything — it's to build systems that are **reliable, maintainable, and appropriate for their context**.

---

*This decision reflects my specific context and learning goals. Your environment may have different constraints and requirements.*</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="design" /><summary type="html">The Question</summary></entry></feed>