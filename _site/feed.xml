<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-24T01:47:47-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Natalie Villasana</title><subtitle>DevOps Engineer | navillasa.dev</subtitle><author><name>Natalie Villasana</name><email>navillasa.dev@gmail.com</email></author><entry><title type="html">Multi-Cloud LLM Routing: When Infrastructure Gets Smart About Money</title><link href="http://localhost:4000/blog/multi-cloud-llm-router-journey/" rel="alternate" type="text/html" title="Multi-Cloud LLM Routing: When Infrastructure Gets Smart About Money" /><published>2025-08-24T00:00:00-04:00</published><updated>2025-08-24T00:00:00-04:00</updated><id>http://localhost:4000/blog/multi-cloud-llm-router-journey</id><content type="html" xml:base="http://localhost:4000/blog/multi-cloud-llm-router-journey/">After successfully running a self-hosted mini LLM (gpt4all-j) on Hetzner, I found myself hungry for a bigger challenge. I wanted to combine several of my interests and learning goals into one ambitious project: running Kubernetes across all three major cloud providers, gaining more hands-on experience with ArgoCD, diving deeper into LLM hosting, and finally getting to work with Pulumi and Go (my absolute favorite programming language).

What started as a learning exercise is evolving into an ambitious multi-cloud LLM router that will automatically distribute requests to the cheapest available cluster across AWS, GCP, and Azure. Currently, I have the router working locally and the AWS infrastructure fully provisioned, with the remaining cloud deployments and full multi-cloud integration as the next major milestone.

## The Problem: LLM Cost Optimization at Scale

Large Language Models are expensive to run. Whether you're using OpenAI's API, hosting your own models, or running inference at scale, costs can quickly spiral out of control. The traditional approach involves either sticking with a single cloud provider (hello, vendor lock-in), manually switching between providers (time-consuming and error-prone), or using static routing that doesn't adapt to real-time price changes or performance.

I wanted to build something that could:
- Automatically route to the cheapest healthy cluster in real-time
- Run across multiple clouds for redundancy and cost optimization
- Scale to zero during low usage periods
- Use CPU-only inference with quantized models for maximum cost efficiency
- Provide comprehensive observability for cost tracking and performance monitoring

## The Solution: An Intelligent Multi-Cloud Router

The architecture I built follows this pattern:

```
[Client] ‚îÄ‚îÄHTTPS‚îÄ‚îÄ&gt; [Global Router (Go Application)]
                         ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ              ‚îÇ                ‚îÇ
   HTTPS/mTLS      HTTPS/mTLS       HTTPS/mTLS
          ‚îÇ              ‚îÇ                ‚îÇ
 [Ingress + Argo CD] [Ingress + Argo CD] [Ingress + Argo CD]
   AWS EKS (CPU)        GCP GKE (CPU)       Azure AKS (CPU)
      ‚îÇ                    ‚îÇ                  ‚îÇ
 [llama.cpp pods]    [llama.cpp pods]   [llama.cpp pods]
  (gguf models)        (gguf models)       (gguf models)
      ‚îÇ                    ‚îÇ                  ‚îÇ
 [Prom + Exporter]  [Prom + Exporter]  [Prom + Exporter]
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄmetrics + costs‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
                 [Router cost engine]
```

### The Core Components

**Intelligent Router (Go):** The heart of the system handles real-time cost calculation using the formula `(node_hourly_cost / (tokens_per_sec * 3600)) * overhead_factor * 1000`, along with health monitoring that enforces SLA requirements like latency and queue depth. It uses HMAC and mTLS authentication for security, collects Prometheus metrics for observability, and implements sticky routing to prevent flapping between providers.

**Multi-Cloud Infrastructure (Pulumi):** The infrastructure layer manages AWS EKS with optimized node groups (t3.large instances for cost efficiency), plus ready-to-deploy GCP GKE and Azure AKS configurations. Everything follows consistent networking, security, and naming patterns across clouds, with automated DNS and TLS certificate management.

**GitOps Deployment (ArgoCD):** For deployment, I'm using ArgoCD with an app-of-apps pattern to manage multiple clusters, automated deployments with self-healing capabilities, environment-specific configurations via Kustomize, and platform components like ingress-nginx, cert-manager, and prometheus.

**CPU-Optimized LLM Serving:** The LLM serving layer runs llama.cpp with quantized GGUF models, uses PVC for model caching (download once per node), supports auto-scaling with scale-to-zero capabilities, and includes comprehensive metrics collection.

## Technology Choices and Learning Goals

### Why Go?
Go has become my favorite programming language for several reasons. The concurrency model is perfect for a router handling many simultaneous requests, the static typing catches errors at compile time, compilation is fast which makes iteration during development much quicker, the ecosystem has fantastic libraries for HTTP, metrics, and cloud APIs, and deployment is simple with a single binary and no dependencies.

### Why Pulumi?
Coming from Terraform, I was excited to try Pulumi because you can use a real programming language (Go in this case) instead of HCL, get type safety with IDE support and compile-time checking, use familiar patterns and existing Go knowledge and libraries, and write better tests using real Go testing frameworks.

### Why ArgoCD?
My previous experience with ArgoCD was limited, so this project was perfect for learning GitOps best practices around declarative infrastructure management, multi-cluster management with a single pane of glass for all environments, application lifecycle management with automated deployments and rollbacks, and security features like RBAC and audit trails.

## What I Built

### Project Foundation &amp; Infrastructure
I started with proper Go module structure and comprehensive documentation covering multi-cloud architecture overview, cost calculation methodology, and development and deployment guides. Then I built reusable components for consistent cloud deployments including resource naming conventions, kubeconfig generation for all three cloud providers, and standardized tagging and labeling.

### AWS Implementation &amp; Router Core  
The AWS infrastructure implementation includes full EKS deployment with VPC and networking optimized for cost, node groups using t3.large instances, IAM roles and security policies, and automated ArgoCD installation.

The intelligent router core became the heart of the system with:
- Cost calculation engine for real-time $/1K token calculations
- Health monitoring with SLA enforcement and circuit breaker patterns  
- Request forwarding with streaming support and authentication
- Comprehensive Prometheus metrics for observability

### Deployment &amp; Developer Experience
I created production-grade Kubernetes deployments with an llm-server chart for llama.cpp with model caching and auto-scaling, a platform chart for core infrastructure (ingress, monitoring, certificates), and cloud-specific values optimized for each provider's services.

The GitOps configuration includes complete ArgoCD setup with app-of-apps pattern for scalability, environment-specific overlays, and automated sync policies with self-healing. I also built model performance and cost documentation, a GitHub Actions pipeline with testing and security scanning, and automated Docker image building and deployment.

To make it easy to contribute and deploy, I focused on comprehensive development documentation, interactive setup scripts with a clean UI, and multiple deployment options (local, cloud, development). The setup automation supports one-command deployment with options for local demo testing, full AWS deployment, and development environment setup.

## The Magic: Cost-Aware Routing

The core innovation is the real-time cost calculation that considers: `costPer1K = (nodeHourlyCost / (tokensPerSecond * 3600)) * overheadFactor * 1000`

Here's how it works with real numbers. An AWS t3.large costs $0.0928/hour and delivers about 15 tokens per second, which works out to roughly $0.0017 per 1K tokens. A GCP n1-standard-2 costs $0.0950/hour with 12 TPS, coming to about $0.0022 per 1K tokens. An Azure Standard_D2s_v3 costs $0.0968/hour with 14 TPS, working out to about $0.0019 per 1K tokens.

In this scenario, the router automatically selects AWS, potentially saving 10-20% on inference costs. When you're processing millions of tokens, those savings add up fast.

## Where Things Stand

**Current Status: Development Phase** - The system is working locally with AWS infrastructure provisioned. Cloud deployment across all three providers is the next major milestone.

### Working System
The router compiled successfully and is running locally. Health monitoring is detecting cluster status, API endpoints are responding correctly locally, Prometheus metrics are being collected, the cost engine is ready for real workloads, and AWS infrastructure is fully provisioned and tested.

### Production Architecture  
The production architecture is complete with infrastructure as code using Pulumi, a GitOps deployment pipeline with ArgoCD, professional Helm charts, and comprehensive monitoring and observability.

### Multi-Cloud Foundation
The multi-cloud foundation has AWS infrastructure fully implemented and working, with GCP and Azure patterns established, consistent tooling and naming across clouds, and everything ready for global deployment in the next phase.

## Performance and Cost Expectations

Based on the architecture and testing, TinyLlama 1.1B on t3.large instances should deliver around 15 tokens per second at roughly $0.0017 per 1K tokens, with latency under 1 second for short prompts and 99.9%+ availability with multi-cloud redundancy.

The scale-to-zero benefits are significant: pods scale down to 0 during low usage, models are cached in PVC so there's no re-download cost, and clusters can auto-scale nodes to minimum when not needed.

## Lessons Learned

Go really is fantastic for infrastructure tools. The concurrency model and standard library made building the router straightforward, and I'm convinced it was the right choice. Pulumi's type safety turned out to be a game-changer ‚Äì catching configuration errors at compile time saved hours of debugging that I would have spent with Terraform. ArgoCD scales beautifully, and the app-of-apps pattern makes managing multiple clusters intuitive. CPU inference is surprisingly cost-effective when you use quantized models properly.

From a project management perspective, making incremental commits that tell a story made the project much more maintainable. Writing comprehensive documentation helped clarify the architecture in my own mind. And investing time in setup automation was absolutely worth it ‚Äì the interactive scripts make the project accessible to new contributors.

## What's Next

The foundation is solid and ready for expansion. The immediate priorities are:

**Phase 1: Complete Cloud Deployment**
- Deploy the working system to AWS, GCP, and Azure clusters
- Validate the cost-aware routing across all three providers

**Phase 2: Production Features**
- Add more sophisticated models like Phi-3 and Llama 2 for production workloads
- Implement circuit breaker patterns for improved reliability
- Add request queueing for handling traffic spikes
- Build cost dashboards in Grafana for financial monitoring
- Implement A/B testing capabilities for model experimentation

## From Learning Project to Production System

What started as a way to learn new technologies is becoming a production-ready system that could save significant money for organizations running LLM workloads at scale. The combination of Go's performance, Pulumi's type safety, ArgoCD's GitOps capabilities, and multi-cloud redundancy creates a powerful platform for cost-optimized AI inference.

The project demonstrates that with the right architecture and tooling, you can build sophisticated infrastructure that's both cost-effective and highly available. While still in active development, the foundation is solid and the local testing validates the core concepts.

This has been an incredible journey from a simple self-hosted LLM on Hetzner to a (soon-to-be) global, multi-cloud AI infrastructure platform. The next chapter involves completing the cloud deployment and scaling to production workloads.

---

*The complete source code and documentation is available at: [github.com/navillasa/multi-cloud-llm-router](https://github.com/navillasa/multi-cloud-llm-router)*</content><author><name>Natalie Villasana</name></author><category term="infrastructure" /><category term="llm" /><category term="multi-cloud" /><category term="go" /><category term="kubernetes" /><category term="argocd" /><category term="pulumi" /><category term="aws" /><category term="gcp" /><category term="azure" /><category term="llm" /><category term="infrastructure" /><summary type="html">After successfully running a self-hosted mini LLM (gpt4all-j) on Hetzner, I found myself hungry for a bigger challenge. I wanted to combine several of my interests and learning goals into one ambitious project: running Kubernetes across all three major cloud providers, gaining more hands-on experience with ArgoCD, diving deeper into LLM hosting, and finally getting to work with Pulumi and Go (my absolute favorite programming language).</summary></entry><entry><title type="html">What I Learned From Building LLM Infrastructure After Missing the AI Revolution</title><link href="http://localhost:4000/blog/missed-ai-revolution/" rel="alternate" type="text/html" title="What I Learned From Building LLM Infrastructure After Missing the AI Revolution" /><published>2025-08-20T00:00:00-04:00</published><updated>2025-08-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/missed-ai-revolution</id><content type="html" xml:base="http://localhost:4000/blog/missed-ai-revolution/">I left tech to pursue advertising in summer 2022, before AI really took off. During my time as a copywriter, AI tools became widespread seemingly overnight. Taking on an LLM API project this summer was as much about refreshing my coding skills as it was about demystifying this technology that's reshaping our world.

## From Magic to Reality Check

The first time I used Cursor to write code, my jaw hit the floor. I was genuinely mesmerized watching it generate entire files in seconds. But nothing could've snapped me out of it faster than Claude trying to hardcode database auth into a docker compose file. It was a moment that perfectly captured AI's unique way of oscillating between the self-assured troubleshooting of a senior engineer to some kind of summer intern from hell.

(I just learned what vibe coding is this week by the way ‚Äì a new approach where developers use AI tools to generate code based on natural language prompts. In other words, based on &quot;vibes.&quot; It sounds misleadingly calm and laidback, but at least in my experience, coding with agents has more of a &quot;you're a helicopter parent to a robot&quot; type vibe than the &quot;feet on the desk chillin'&quot; vibe I expected.)

## CPU-Only LLMs: When Sluggishness is Part of the Charm

For my LLM project, I wanted to understand the fundamentals without breaking the bank. I chose Hetzner for hosting and looked into CPU-only models. Part of the appeal of self-hosting your own LLM is that you can have chats without your data going to Google or OpenAI. But despite legitimate concerns about privacy, the vast majority of people have no option but use the major AI tools like ChatGPT, CoPilot, Gemini, etc. Their dominance in the market, and therefore convenience factor ‚Äì not to mention the technical and economic hurdles of setting up your own private model ‚Äì mean that privacy, as usual, is forced to take a back seat to practicality.

That said, the prevalence of mainstream tools doesn‚Äôt mean innovation or alternatives are out of reach. At the end of the day, experimenting and imagining new ways of doing things is what engineering is all about. While my app is charmingly unusable in its slowness, I built the infrastructure so one could easily switch out the models, inference engine, and hosting environment. It was an exercise in understanding how these applications actually work.

## Demystifying the Components

**A LLM** (Large Language Model) is like a master chef who has trained by tasting thousands of dishes and internalized patterns of what makes food delicious.

In reality, a model is just a giant file of numbers ‚Äì millions or billions of numerical parameters encoding everything learned during training. 

**Inference** is using the model to answer questions, generate images, etc. Inference is the actual cooking process. When you ask the model to generate text, it uses those learned patterns to make decisions.

The **inference engine** is the kitchen itself: the computational environment that makes it possible for our chef to get to work.

It wasn't until I saw the price tags on the GCP Compute Engine page for GPU-resourced VMs that I began to understand just how much power LLMs need ‚Äì even on a small scale!

I had been researching ratings and reviews of different models, was hearing good things about the model Mistral 7b, and was planning on using it for my app.

Then I learned that a &quot;budget&quot; or smaller version of Mistral 7b would still require ~8-12 GB of VRAM (dedicated GPU memory) to function properly. And I learned that an AWS instance that could support that could run you upwards of $300/month.

That's for a smaller version of Mistral 7b, mind you, where its inference is scaled down. The &quot;Mid-tier GPU inference&quot; ‚Äì the FP16 version ‚Äì would require ~24GB of VRAM for smooth inference. Which could run you upwards of $3k/month for the appropriate AWS instance.

So yes, that's why even though I definitely, totally have $3k to dish out monthly for my LLM pet project, I opted for the slowest, tiniest (but highly recommended) model I could find: GPT4All.

All of this put into perspective the computational power needed for AI on an enterprise scale. And when you think about companies training models on the entire internet's worth of data, the scale is truly mind-boggling.

## What This Project Taught Me

### 1. The Infrastructure Reality
Running even a small LLM gave me deep appreciation for the engineering challenges at scale. Meaning that optimization is no joke.

### 2. The Experimentation Framework
In an ideal world, I would have loved to deploy dozens of different models and built dashboards comparing cost, speed, accuracy, and specialization. (I know those dashboards exist somewhere and still would love to see them. üëÄ)

### 3. It's Time to Demystify AI
There's a lot of misunderstanding around what AI is as a technology. In order for more people to have a say in how AI shapes our world, there needs to be more readily available, accurate information about what it is and how it works. There's a need to demystify AI because it's here whether we like it or not. It's actually crucial for more people to understand how AI works because of the sheer magnitude with which it's affecting our world. It's changing our daily interactions, our work, and ultimately it's reshaping the entire economy in ways we haven't fully felt yet.

The comparison to the assembly line or steam engine isn't hyperbole. While the full consequences aren't visible yet, the transformation is already underway. Understanding how these systems work, their limitations, and how they can be changed, isn't just a question of technical curiosity, it's a collective necessity.

### 4. Creative Engineering Exists
I've always been drawn to the creative aspects of engineering: the problem-solving, the elegant solutions, the way good infrastructure feels like art. In my opinion, AI expands that human creative potential ‚Äì because its power comes from human creativity and work in the first place. The question becomes ‚Äì like with any epoch-shaping technology ‚Äì who has oversight and calls the shots on how AI is used at the end of the day?

## The Rip Van Winkle Effect
#### (Is that a dated reference? Besides in the obvious way.)
This has been a fascinating time-skip back into tech. Missing the initial AI explosion and returning to discover a self-coding VSCode has been eye-opening to say the least.

I think AI is an incredible breakthrough with massive both creative and destructive potential. The direction it goes will be determined by who controls it in the future on a massive scale. I think the more people who learn about AI, demystify AI, and imagine ways to use it as a tool for collective good, the better off we all will be.

Understanding AI doesn't just mean understanding the math behind it or its technical functionality, but also means understanding the wide-reaching social impacts it has and will have in many aspects of life. Imagining the future of AI isn't a responsibility reserved for just MLOps engineers and researchers, this is something everyone has a stake in.

The future feels wide open, and I'm eager to keep learning ‚Äì about LLMs, about infrastructure, and about the meeting point of creativity and automation. There's something energizing about being back in tech during such a transformative moment.

So even if my LLM API is so slow that it hurts, it's reminded me that understanding these systems isn't optional anymore. It's part of playing an active role in the world we're all building.</content><author><name>Natalie Villasana</name></author><category term="ai" /><category term="llm" /><category term="infrastructure" /><category term="ai" /><category term="llm" /><category term="infrastructure" /><summary type="html">I left tech to pursue advertising in summer 2022, before AI really took off. During my time as a copywriter, AI tools became widespread seemingly overnight. Taking on an LLM API project this summer was as much about refreshing my coding skills as it was about demystifying this technology that‚Äôs reshaping our world.</summary></entry><entry><title type="html">The CI/CD Pipeline I Wish I‚Äôd Built Sooner</title><link href="http://localhost:4000/blog/gitops-cicd-pipeline/" rel="alternate" type="text/html" title="The CI/CD Pipeline I Wish I‚Äôd Built Sooner" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/gitops-cicd-pipeline</id><content type="html" xml:base="http://localhost:4000/blog/gitops-cicd-pipeline/">When I started building my [TV Hub project]({% link _posts/2025-08-19-tv-hub-project-overview.md %}), I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here's the GitOps pipeline I wish I'd built from day one.

## The Problem

My original setup was very simple and anxiety-inducingly fragile. Every git push triggered a full deployment to production. No gates, no testing environment, just vibes.

When a color-changing button broke the frontend one too many times, I realized I needed a system that could move fast without breaking things. The solution needed to automatically deploy tested code while preventing untested changes from reaching &quot;production.&quot;

## The Solution: Two-Speed GitOps

I ended up with a hybrid approach that treats development and production very differently:

```
git push ‚Üí GitHub Actions ‚Üí new images ‚Üí dev deploys automatically
                                      ‚Üí prod requires manual promotion
```

Here's how it works:

**Development environment:** Every successful build auto-deploys. Fast feedback, immediate testing, maximum iteration speed.

**Production environment:** Requires explicit promotion of a tested version. Slower, deliberate, with human accountability.

## How the Pipeline Works

When I push code to main, GitHub Actions runs the full test suite, builds Docker images, and tags them with a date-based version like `v20250819-f3b8541`. If tests pass, it automatically updates the development Kubernetes manifests and pushes the change back to git.

ArgoCD watches the git repo and syncs any changes to the development cluster within about 30 seconds. I can immediately test the new version at the dev URL.

For production, there's no automatic deployment. Instead, I run a promotion script:

```bash
# See what versions are available
./scripts/promote-to-prod.sh

# Promote a tested version to production  
./scripts/promote-to-prod.sh v20250819-f3b8541
```

The script shows me exactly what will change, asks for confirmation, then updates the production manifests and pushes to git. ArgoCD syncs the change to production.

## Technical Choices

I made several specific decisions that shaped how this pipeline works:

**Date-based versioning:** Instead of incrementing numbers, I use `v$(date +%Y%m%d)-$(git-hash)`. This makes it obvious when a version was created and provides a direct link back to the source code. Much easier for operations than trying to remember what v1.47 contained.

**Kustomize overlays:** The same base Kubernetes manifests work for both environments, with environment-specific patches. Development gets relaxed resource limits and debug logging; production gets proper resource constraints, and theoretically should get structured logs.

**ArgoCD split configuration:** The development ArgoCD app has `automated: true` for immediate deployments. The production app has automated sync disabled, requiring manual intervention for every change.

**Container registry strategy:** Both environments pull from the same registry, just with different tags. This ensures what I test in dev is exactly what deploys to prod‚Äîno separate build processes or subtle differences.

## What I Could Have Done Differently

**Full automation:** I could have set up comprehensive integration tests and deployed to production automatically if everything passes. The tooling exists‚ÄîArgo Rollouts for progressive delivery, sophisticated test suites, automated rollback triggers.

I chose manual promotion instead because I wanted deliberate production changes. This is my portfolio project after all! The slight overhead of manual promotion forces me to actually test changes in dev and think about the impact.

**Branch-based environments:** Many teams use feature branches for feature environments, develop branch for staging, and main branch for production. I went with a simpler model where main always goes to dev, and production gets explicit promotions.

The branch approach would have been more &quot;textbook correct&quot; but would have required more operational overhead managing multiple environments and branch policies. For a portfolio project, the current approach demonstrates the core GitOps concepts without unnecessary complexity.

**External tools:** I could have used Flux instead of ArgoCD, or Tekton instead of GitHub Actions, or Jenkins X for the whole pipeline. The GitHub Actions + ArgoCD combination is incredibly common in the industry, well-documented, and has good operational tooling.

## Results

The manual promotion gate adds maybe 2 minutes to the deployment process but provides huge peace of mind. I can deploy more frequently because I'm confident each change has been tested and validated.

For a team environment, I'd probably add Slack notifications for deployments and automated integration tests against the dev environment. But for a solo project, this pipeline hits the sweet spot of safety, speed, and simplicity.

The beauty of GitOps is that these patterns scale. The same basic structure works whether you're promoting manually like I do, or using sophisticated automated promotion with comprehensive test coverage. The deployment mechanism stays consistent‚Äîonly the automation boundaries change.

---

*This pipeline powers the [TV Hub project](http://tv-hub.navillasa.dev) with full source available in the [GitHub repo](https://github.com/navillasa/tv-dashboard-k8s). You can see real deployment metrics in the [live monitoring dashboard](https://monitoring.navillasa.dev/d/e0c978bd-6077-403e-ab3b-ba03f4b34962/tv-hub-business-intelligence-dashboard).*</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="gitops" /><category term="ci-cd" /><summary type="html">When I started building my TV Hub project, I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here‚Äôs the GitOps pipeline I wish I‚Äôd built from day one.</summary></entry><entry><title type="html">Go Big, Stay Simple: Building Production-Grade Infrastructure for a Tiny App</title><link href="http://localhost:4000/blog/tv-hub-project-overview/" rel="alternate" type="text/html" title="Go Big, Stay Simple: Building Production-Grade Infrastructure for a Tiny App" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/tv-hub-project-overview</id><content type="html" xml:base="http://localhost:4000/blog/tv-hub-project-overview/">## The Paradox of Portfolio Projects

I faced a classic engineering dilemma: **build something complex that needs sophisticated infrastructure, or build something simple and add sophisticated infrastructure anyway?**

After cycling through ideas (distributed microservices, ML pipelines, IoT dashboards) I kept returning to one idea: a TV show aggregator. Because it was an app with some appeal and usefulness beyond demonstrating its own tech stack. The app itself is straightforward: fetch trending shows from TMDB and TVmaze APIs, rank by popularity, display in a clean interface.

**But there's a twist.** I deployed it with enterprise-grade DevOps practices that would typically serve a team of a 50+ engineers. Or at least a dozen.

## Strategic Design Decisions

### **Simple App, Complex Infrastructure**

I specifically chose a simple application to isolate and focus on infrastructure complexity. Here's why this approach worked:

**Cognitive Load Management**: Managing ArgoCD, Vault integration, and Prometheus monitoring is challenging enough without debugging distributed system race conditions simultaneously.

**Clear Separation of Concerns**: When something breaks, I can immediately identify whether it's an application bug (less likely, given the simplicity) or an infrastructure configuration issue (very likely, given the learning curve).

**Real-World Simulation**: Most production systems start simple and grow complex. This project demonstrates the infrastructure foundation that scales.

## Technical Architecture

The application architecture is intentionally minimal:

```
React Frontend ‚Üê‚Üí Node.js API ‚Üê‚Üí PostgreSQL ‚Üê‚Üí External APIs
```

The **infrastructure architecture** tells a different story:

```
GitHub ‚Üí Actions ‚Üí GCR ‚Üí ArgoCD ‚Üí GKE Autopilot
    ‚Üì                        ‚Üì
Terraform ‚Üí GCP Resources   Vault ‚Üí External Secrets ‚Üí K8s Secrets
    ‚Üì                        ‚Üì
Monitoring ‚Üê Prometheus ‚Üê Applications ‚Üí Logs
```

### **Production-Grade Components**

**GitOps with ArgoCD**: Declarative deployments with dev/staging/prod environment promotion. Changes flow through Git, not `kubectl apply`.

**Infrastructure as Code**: Terraform manages GKE clusters, static IPs, DNS records, and IAM policies.

**Secrets Management**: HashiCorp Vault with External Secrets Operator. API keys and database credentials never touch Git or container images.

**Multi-Environment Overlays**: Kustomize overlays handle environment-specific configurations. Production can use different resource limits, replica counts, and ingress rules than development.

**Observability Stack**: Prometheus metrics collection with Grafana dashboards. Custom application metrics track API response times and cache hit rates.

## What I Learned

### **1. Mental Model Clarity**

Complex infrastructure demands clear mental models. When ArgoCD shows &quot;OutOfSync&quot; status, I need to immediately understand:
- Which Git commit should be deployed?
- What Kustomize transformations apply?
- How do external secrets affect pod startup?

**Practical Impact**: Reduced debugging time from hours to minutes by maintaining clear architectural diagrams and understanding component dependencies.

### **2. Architectural Trade-offs**

Every decision has implications. For example: [Vault as ArgoCD Application]({{ '/blog/vault-gitops/' | relative_url }}) explores why I kept secrets management outside GitOps workflows.

**Key Trade-off**: GitOps consistency vs. bootstrap complexity. Pure GitOps purists would manage Vault through ArgoCD, but circular dependencies make this operationally complex.

**Decision Framework**: Evaluate based on operational simplicity, recovery scenarios, and production patterns, not theoretical purity.

### **3. Systematic Debugging**

Production systems fail in creative ways. In [The Cluster Doctor]({{ '/blog/debugging-502-errors/' | relative_url }}) blog post I demonstrate my approach to systematic troubleshooting.

**Core Principle**: Most failures are configuration mismatches, not infrastructure failures. Port conflicts, service discovery issues, and protocol mismatches accounted for the vast majority of my deployment problems.

## Why This Approach Works

### **For Learning**
Starting with a simple application let me focus on infrastructure concepts without application complexity interference. I could experiment with GitOps workflows, monitoring configurations, and security policies without worrying about business logic bugs. For the most part :)

### **For Demonstrating**
**Systems Thinking**: Understanding how components interact across multiple layers (application, orchestration, infrastructure, security).

**Production Readiness**: Moving beyond &quot;it works on my machine&quot; to &quot;it works reliably in production with proper monitoring, security, and deployment practices.&quot;

**Technology Integration**: Connecting multiple tools (ArgoCD, Vault, Prometheus, Terraform) into cohesive workflows rather than using them in isolation.

**Decision Making**: Choosing appropriate architectural patterns based on trade-offs. A lot more fun, and difficult, than following tutorials.

## Current Status
[Live application](http://tv-hub.navillasa.dev) with full GitOps deployment, multi-environment promotion, and comprehensive monitoring.

**Next Phases**: Cost optimization dashboard, Redis caching layer, and advanced alerting rules.

## The Meta-Learning

This project demonstrates that I can build production-ready infrastructure, make informed architectural decisions, and systematically debug complex systems.

More than learning how to use individual tools, I learned how they integrate, _when_ to use them, and what trade-offs each decision creates.

Building infrastructure for a simple application taught me more about production DevOps than building a complex application with simple deployment ever could.</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="architecture" /><category term="kubernetes" /><category term="argocd" /><category term="terraform" /><category term="gitops" /><category term="gke" /><category term="portfolio" /><summary type="html">The Paradox of Portfolio Projects</summary></entry><entry><title type="html">The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s</title><link href="http://localhost:4000/blog/debugging-502-errors/" rel="alternate" type="text/html" title="The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/debugging-502-errors</id><content type="html" xml:base="http://localhost:4000/blog/debugging-502-errors/">## The Problem Pattern

Throughout this project, I've hit the same type of issue repeatedly:
- **Frontend** looking for `dev-backend-service` instead of `prod-backend-service`
- **ArgoCD ingress** using port 443 instead of port 80
- **Vault** connection errors due to service discovery issues
- **External Secrets** pointing to wrong Vault paths

**The common thread**: 502 Bad Gateway errors caused by communication mismatches.

## The 502 Error: What It Really Means

```
502 Bad Gateway = &quot;I can reach something, but it's not responding correctly&quot;
```

**This is different from:**
- **404**: &quot;I can't find anything at this path&quot;
- **503**: &quot;Service is temporarily unavailable&quot;  
- **Connection timeout**: &quot;I can't reach anything at all&quot;

**502 specifically means**: The proxy/load balancer reached a backend, but the backend response was invalid.

## The Systematic Debugging Approach

### Step 1: Identify the Communication Chain

Every 502 has a path like this:
```
Client ‚Üí Load Balancer ‚Üí Service ‚Üí Pod
```

Find where it breaks by checking each link.

### Step 2: Check Backend Health in Load Balancer

```bash
# Get detailed ingress status
kubectl describe ingress &lt;ingress-name&gt; -n &lt;namespace&gt;

# Look for backend health status
# Example output:
ingress.kubernetes.io/backends: {
  &quot;k8s1-backend-service-443&quot;:&quot;UNHEALTHY&quot;  # ‚Üê The smoking gun!
}
```

**Key indicators:**
- `UNHEALTHY` = Backend isn't responding on expected port/protocol
- `HEALTHY` = Backend is fine, look elsewhere

## Pro Tips for Faster Debugging

### 1. **Use Port-Forward for Direct Testing**
```bash
# Bypass ingress/service and test pod directly
kubectl port-forward pod/&lt;pod-name&gt; 8080:8080
curl http://localhost:8080/health
```

### 2. **Check Multiple Namespaces**
```bash
# Service in wrong namespace is super common
kubectl get services --all-namespaces | grep &lt;service-name&gt;
```

The key insight: Most 502 errors are configuration mismatches, not infrastructure failures. A systematic approach beats random troubleshooting every time.</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="debugging" /><category term="devops" /><category term="kubernetes" /><category term="502-errors" /><category term="troubleshooting" /><category term="nginx" /><category term="ingress" /><summary type="html">The Problem Pattern</summary></entry><entry><title type="html">Why I Didn‚Äôt Make Vault an ArgoCD Application</title><link href="http://localhost:4000/blog/vault-gitops/" rel="alternate" type="text/html" title="Why I Didn‚Äôt Make Vault an ArgoCD Application" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/vault-gitops</id><content type="html" xml:base="http://localhost:4000/blog/vault-gitops/">## The Question

During the GitOps implementation, I considered whether HashiCorp Vault should be managed as an ArgoCD application alongside the other workloads. This post explains why I chose to keep Vault outside of the GitOps workflow.

## The Temptation

At first glance, managing Vault through ArgoCD seems _pretty_ cool:

- **Consistency**: Everything else is managed by ArgoCD
- **Version Control**: Vault configuration tracked in Git
- **Declarative**: Infrastructure as Code principles
- **Automation**: Automated deployments and updates

## The Bootstrap Problem

The fundamental issue is a **circular dependency**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    needs    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    needs    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ArgoCD    ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ External Secrets ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Vault   ‚îÇ
‚îÇ             ‚îÇ             ‚îÇ    Operator      ‚îÇ             ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚ñ≤                                                            ‚îÇ
       ‚îÇ                    manages (if GitOps)                     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**The problem**: If ArgoCD manages Vault, then ArgoCD depends on Vault (for secrets) which depends on ArgoCD (for deployment). This creates an unresolvable bootstrap dependency.

## Infrastructure Layers

I solved this by establishing clear **infrastructure layers**:

### Layer 0: Foundation Infrastructure
- **GKE Cluster** (Terraform)
- **Static IPs** (Terraform)
- **Vault** (kubectl apply)
- **External Secrets Operator** (Helm)

### Layer 1: GitOps Platform
- **ArgoCD** (kubectl apply)
- **ClusterSecretStore** (ArgoCD)

### Layer 2: Applications
- **tv-dashboard-dev** (ArgoCD)
- **tv-dashboard-prod** (ArgoCD)
- **monitoring-stack** (ArgoCD)

## Alternative Approaches Considered

### 1. Manual Sync Only
```yaml
# vault-app.yaml
spec:
  syncPolicy:
    automated: null  # No auto-sync to avoid bootstrap issues
```

**Pros**: Vault config in Git  
**Cons**: Manual intervention required, defeats GitOps automation

### 2. App-of-Apps Pattern
```yaml
# bootstrap-app.yaml - deployed manually
# Manages vault-app.yaml and argocd-apps.yaml
```

**Pros**: Complete GitOps coverage  
**Cons**: Complex bootstrap sequence, harder to debug

### 3. External Vault
Use a managed secret service (Google Secret Manager, AWS Secrets Manager)

**Pros**: No bootstrap problem, less operational overhead  
**Cons**: Vendor lock-in, less learning value for this project

## The Decision: Keep Vault Outside GitOps

**Reasons**:

1. **Operational Simplicity**: Vault is foundational infrastructure that should be stable and simple to manage
2. **Recovery Scenarios**: If ArgoCD fails, we can still access Vault to recover secrets
3. **Bootstrap Clarity**: Clear separation between foundation and application layers
4. **Production Patterns**: Many organizations treat secret management as Layer 0 infrastructure

## Production Considerations

In production environments, you might see:

### At Large Organizations
- **Dedicated Vault clusters** managed by platform teams
- **Vault-as-a-Service** provided to application teams
- **Manual deployment** with infrastructure automation (Terraform)

### At Cloud-Native Shops
- **Managed secret services** (AWS Secrets Manager, etc.)
- **External Secrets Operator** connecting to cloud providers
- **No self-hosted Vault** at all

### In GitOps Purist Repos
- **Everything in Git** including Vault
- **Complex bootstrap procedures** with operator dependencies
- **Acceptance of operational complexity** for consistency

## Key Takeaway

**Not everything needs to be in GitOps.** The right architectural boundary depends on:

- **Operational complexity** vs **consistency benefits**
- **Bootstrap dependencies** and **recovery scenarios**  
- **Team structure** and **operational expertise**
- **Compliance requirements** and **audit trails**

For my TV Dashboard project, keeping Vault as foundational infrastructure provided the right balance of simplicity and functionality while still demonstrating modern secret management practices.

## What I Learned

1. **Architectural boundaries matter** - not every tool fits every pattern
2. **Bootstrap dependencies** are real constraints in system design
3. **Operational simplicity** often trumps theoretical consistency
4. **Production patterns** should inform learning project decisions

The goal isn't to GitOps-ify everything ‚Äî it's to build systems that are **reliable, maintainable, and appropriate for their context**.

---

*This decision reflects my specific context and learning goals. Your environment may have different constraints and requirements.*</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="design" /><summary type="html">The Question</summary></entry></feed>