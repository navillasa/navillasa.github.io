<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-20T03:29:22-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Natalie Villasana</title><subtitle>DevOps Engineer | navillasa.dev</subtitle><author><name>Natalie Villasana</name><email>navillasa.dev@gmail.com</email></author><entry><title type="html">What I Learned From Building LLM Infrastructure After Missing the AI Revolution</title><link href="http://localhost:4000/blog/missed-ai-revolution/" rel="alternate" type="text/html" title="What I Learned From Building LLM Infrastructure After Missing the AI Revolution" /><published>2025-08-20T00:00:00-04:00</published><updated>2025-08-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/missed-ai-revolution</id><content type="html" xml:base="http://localhost:4000/blog/missed-ai-revolution/">I left tech to pursue advertising in summer 2022, before AI really took off. During my time as a copywriter, AI tools became widespread seemingly overnight. Taking on an LLM API project was as much about refreshing my coding skills as it was about demystifying this technology that's reshaping everything.

## From Magic to Reality Check

The first time I used Cursor to write code, my jaw hit the floor. I was genuinely mesmerized watching it generate entire files in seconds. But nothing could've snapped me out of it faster than Claude trying to hardcode database auth into a Docker Compose file. It was a moment that perfectly captured AI's unique way of oscillating between the self-assured troubleshooting of a senior engineer and wild flailing of someone who's never seen a codebase before.

(I just learned what vibe coding is this week by the way. It sounds calm and laidback, but trust me the vibes can go from immaculate to egregious faster than you can push to prod.)

## CPU-Only LLMs: When Sluggishness is Part of the Charm

For my LLM project, I wanted to understand the fundamentals without breaking the bank. I chose Hetzner for hosting and looked into CPU-only models. I suppose the appeal of self-hosting your own LLM is that you can have chats without your data going to Google or OpenAI, but the cultural convenience and prevalence of ChatGPT and Gemini et al, and the complexity of running private instances obviously tips the scales against valid privacy concerns.

But at the end of the day, experimenting and imagining other ways of doing things is what engineering is all about. While my app is charmingly unusable in its slowness, I built the infrastructure so one could easily switch out the models, inference engine, and hosting environment. It was an exercise in understanding how these applications actually work.

## Demystifying the Components

**A LLM** (Large Language Model) is like a master chef who has trained by tasting thousands of dishes and internalized patterns of what makes food delicious.

In reality, a model is just a giant file of numbers ‚Äì millions or billions of numerical parameters encoding everything learned during training. 

**Inference** is using the model to answer questions, generate images, etc. Inference is the actual cooking process. When you ask the model to generate text, it uses those learned patterns to make decisions.

The **inference engine** is the kitchen itself: the computational environment that makes it possible for our chef to get to work.

It wasn't until I saw the price tags on the GCP Compute Engine page for GPU-resourced VMs that I began to understand just how much power LLMs need ‚Äì even on a small scale!

I had been researching ratings and reviews of different models, was hearing good things about the model Mistral 7b, and was planning on using it for my app.

Then I learned that a &quot;budget&quot; or smaller version of Mistral 7b would still require ~8-12 GB of VRAM (dedicated GPU memory) to function properly. And I learned that an AWS instance that could support that could run you upwards of $300/month.

That is for a smaller, &quot;quantized&quot; version of Mistral 7b, mind you, where its inference is scaled down. The &quot;Mid-tier GPU inference&quot; ‚Äì the FP16 version ‚Äì would require ~24GB of VRAM for smooth inference. Which could run you upwards of $3k/month for the appropriate AWS instance.

(I put &quot;quantized&quot; in quotes not to denote sarcasm or something but actually to flag that I would probably butcher the explanation of what it means, so I'll encourage you to find another blog post where someone more qualified can do that.)

So yes, that's why even though I definitely totally have $3k to dish out monthly for my LLM pet project, I opted for the slowest, tiniest (but highly recommended) model I could find: GPT4All.

All of this put into perspective the computational power needed for AI on an enterprise scale. And when you think about companies training models on the entire internet's worth of data, the scale is truly mind-boggling.

## What This Project Taught Me

### 1. The Infrastructure Reality
Running even a small LLM gave me deep appreciation for the engineering challenges at scale. Meaning optimization is no joke.

### 2. The Experimentation Framework
In an ideal world, I would have loved to deploy dozens of different models and built dashboards comparing cost, speed, accuracy, and specialization. (I know those dashboards exist somewhere, would love to see them üëÄ)

### 3. It's Up to Us to Understand AI
We all need to demystify AI because it's here whether we like it or not. It's actually crucial for everyone to understand these systems because of the sheer magnitude that it is affecting our world. It's changing our daily interactions, our work, and ultimately it's reshaping the entire economy in ways we haven't fully felt yet.

The comparison to the assembly line or steam engine isn't hyperbole. While the full consequences aren't visible yet, the transformation is already underway. Understanding how these systems work, their limitations, and their capabilities isn't just technical curiosity, it's really a collective necessity.

### 4. Creative Engineering Exists
I've always been drawn to the creative aspects of engineering: the problem-solving, the elegant solutions, the way good infrastructure feels like art. It's fraught topic, but in my opinion, AI expands that human creative potential ‚Äì because its power comes from human creativity and work in the first place. The question becomes ‚Äì like with any epoch-shifting technology ‚Äì who has oversight and calls the shots on how AI is used at the end of the day?

## The Rip Van Winkle Effect
#### (Is that a dated reference? Besides in the obvious way.)
This has been a fascinating time-skip back into tech. Missing the initial AI explosion and returning to discover a self-writing VSCode has been eye-opening to say the least.

I think AI is an incredible breakthrough with massive both creative and destructive potential. The direction it goes will be deteremined by who controls it in the future on a massive scale. I think the more people who learn about AI, demystify AI, and imagine ways to use it as a tool for collective good, the better off we all will be. And maybe it doesn't need to be said, but to be clear, imagining the future of AI isn't a responsibility reserved for just MLOps engineers or people in tech ‚Äì this is something everyone has a stake in.

The future feels wide open, and I'm eager to keep learning ‚Äì about LLMs, about infrastructure, and about the meeting point of creativity and automation. There's something energizing about being back in tech during such a transformative moment.

So even if my LLM API is so slow that it kind of hurts, it's reminded me that understanding these systems isn't optional anymore. It's part of playing an active role in the world we're all building.</content><author><name>Natalie Villasana</name></author><category term="ai" /><category term="llm" /><category term="infrastructure" /><category term="ai" /><category term="llm" /><category term="infrastructure" /><summary type="html">I left tech to pursue advertising in summer 2022, before AI really took off. During my time as a copywriter, AI tools became widespread seemingly overnight. Taking on an LLM API project was as much about refreshing my coding skills as it was about demystifying this technology that‚Äôs reshaping everything.</summary></entry><entry><title type="html">The CI/CD Pipeline I Wish I‚Äôd Built Sooner</title><link href="http://localhost:4000/blog/gitops-cicd-pipeline/" rel="alternate" type="text/html" title="The CI/CD Pipeline I Wish I‚Äôd Built Sooner" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/gitops-cicd-pipeline</id><content type="html" xml:base="http://localhost:4000/blog/gitops-cicd-pipeline/">When I started building my [TV Hub project]({% link _posts/2025-08-19-tv-hub-project-overview.md %}), I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here's the GitOps pipeline I wish I'd built from day one.

## The Problem

My original setup was very simple and anxiety-inducingly fragile. Every git push triggered a full deployment to production. No gates, no testing environment, just vibes.

When a color-changing button broke the frontend one too many times, I realized I needed a system that could move fast without breaking things. The solution needed to automatically deploy tested code while preventing untested changes from reaching &quot;production.&quot;

## The Solution: Two-Speed GitOps

I ended up with a hybrid approach that treats development and production very differently:

```
git push ‚Üí GitHub Actions ‚Üí new images ‚Üí dev deploys automatically
                                      ‚Üí prod requires manual promotion
```

Here's how it works:

**Development environment:** Every successful build auto-deploys. Fast feedback, immediate testing, maximum iteration speed.

**Production environment:** Requires explicit promotion of a tested version. Slower, deliberate, with human accountability.

## How the Pipeline Works

When I push code to main, GitHub Actions runs the full test suite, builds Docker images, and tags them with a date-based version like `v20250819-f3b8541`. If tests pass, it automatically updates the development Kubernetes manifests and pushes the change back to git.

ArgoCD watches the git repo and syncs any changes to the development cluster within about 30 seconds. I can immediately test the new version at the dev URL.

For production, there's no automatic deployment. Instead, I run a promotion script:

```bash
# See what versions are available
./scripts/promote-to-prod.sh

# Promote a tested version to production  
./scripts/promote-to-prod.sh v20250819-f3b8541
```

The script shows me exactly what will change, asks for confirmation, then updates the production manifests and pushes to git. ArgoCD syncs the change to production.

## Technical Choices

I made several specific decisions that shaped how this pipeline works:

**Date-based versioning:** Instead of incrementing numbers, I use `v$(date +%Y%m%d)-$(git-hash)`. This makes it obvious when a version was created and provides a direct link back to the source code. Much easier for operations than trying to remember what v1.47 contained.

**Kustomize overlays:** The same base Kubernetes manifests work for both environments, with environment-specific patches. Development gets relaxed resource limits and debug logging; production gets proper resource constraints, and theoretically should get structured logs.

**ArgoCD split configuration:** The development ArgoCD app has `automated: true` for immediate deployments. The production app has automated sync disabled, requiring manual intervention for every change.

**Container registry strategy:** Both environments pull from the same registry, just with different tags. This ensures what I test in dev is exactly what deploys to prod‚Äîno separate build processes or subtle differences.

## What I Could Have Done Differently

**Full automation:** I could have set up comprehensive integration tests and deployed to production automatically if everything passes. The tooling exists‚ÄîArgo Rollouts for progressive delivery, sophisticated test suites, automated rollback triggers.

I chose manual promotion instead because I wanted deliberate production changes. This is my portfolio project after all! The slight overhead of manual promotion forces me to actually test changes in dev and think about the impact.

**Branch-based environments:** Many teams use feature branches for feature environments, develop branch for staging, and main branch for production. I went with a simpler model where main always goes to dev, and production gets explicit promotions.

The branch approach would have been more &quot;textbook correct&quot; but would have required more operational overhead managing multiple environments and branch policies. For a portfolio project, the current approach demonstrates the core GitOps concepts without unnecessary complexity.

**External tools:** I could have used Flux instead of ArgoCD, or Tekton instead of GitHub Actions, or Jenkins X for the whole pipeline. The GitHub Actions + ArgoCD combination is incredibly common in the industry, well-documented, and has good operational tooling.

## Results

The manual promotion gate adds maybe 2 minutes to the deployment process but provides huge peace of mind. I can deploy more frequently because I'm confident each change has been tested and validated.

For a team environment, I'd probably add Slack notifications for deployments and automated integration tests against the dev environment. But for a solo project, this pipeline hits the sweet spot of safety, speed, and simplicity.

The beauty of GitOps is that these patterns scale. The same basic structure works whether you're promoting manually like I do, or using sophisticated automated promotion with comprehensive test coverage. The deployment mechanism stays consistent‚Äîonly the automation boundaries change.

---

*This pipeline powers the [TV Hub project](http://tv-hub.navillasa.dev) with full source available in the [GitHub repo](https://github.com/navillasa/tv-dashboard-k8s). You can see real deployment metrics in the [live monitoring dashboard](https://monitoring.navillasa.dev/d/e0c978bd-6077-403e-ab3b-ba03f4b34962/tv-hub-business-intelligence-dashboard).*</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="gitops" /><category term="ci-cd" /><summary type="html">When I started building my TV Hub project, I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here‚Äôs the GitOps pipeline I wish I‚Äôd built from day one.</summary></entry><entry><title type="html">Go Big, Stay Simple: Building Production-Grade Infrastructure for a Simple App</title><link href="http://localhost:4000/blog/tv-hub-project-overview/" rel="alternate" type="text/html" title="Go Big, Stay Simple: Building Production-Grade Infrastructure for a Simple App" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/tv-hub-project-overview</id><content type="html" xml:base="http://localhost:4000/blog/tv-hub-project-overview/">&lt;h2 id=&quot;the-paradox-of-portfolio-projects&quot;&gt;The Paradox of Portfolio Projects&lt;/h2&gt;

&lt;p&gt;I faced a classic engineering dilemma: &lt;strong&gt;build something complex that needs sophisticated infrastructure, or build something simple and add sophisticated infrastructure anyway?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After cycling through ideas (distributed microservices, ML pipelines, IoT dashboards) I kept returning to one idea: a TV show aggregator. Because it was an app with some appeal and usefulness beyond demonstrating its own tech stack. The app itself is straightforward: fetch trending shows from TMDB and TVmaze APIs, rank by popularity, display in a clean interface.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But there‚Äôs a twist.&lt;/strong&gt; I deployed it with enterprise-grade DevOps practices that would typically serve a team of a 50+ engineers. Or at least a dozen.&lt;/p&gt;

&lt;h2 id=&quot;strategic-design-decisions&quot;&gt;Strategic Design Decisions&lt;/h2&gt;

&lt;h3 id=&quot;simple-app-complex-infrastructure&quot;&gt;&lt;strong&gt;Simple App, Complex Infrastructure&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;I specifically chose a simple application to isolate and focus on infrastructure complexity. Here‚Äôs why this approach worked:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cognitive Load Management&lt;/strong&gt;: Managing ArgoCD, Vault integration, and Prometheus monitoring is challenging enough without debugging distributed system race conditions simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clear Separation of Concerns&lt;/strong&gt;: When something breaks, I can immediately identify whether it‚Äôs an application bug (less likely, given the simplicity) or an infrastructure configuration issue (very likely, given the learning curve).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-World Simulation&lt;/strong&gt;: Most production systems start simple and grow complex. This project demonstrates the infrastructure foundation that scales.&lt;/p&gt;

&lt;h2 id=&quot;technical-architecture&quot;&gt;Technical Architecture&lt;/h2&gt;

&lt;p&gt;The application architecture is intentionally minimal:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;React Frontend ‚Üê‚Üí Node.js API ‚Üê‚Üí PostgreSQL ‚Üê‚Üí External APIs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;infrastructure architecture&lt;/strong&gt; tells a different story:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GitHub ‚Üí Actions ‚Üí GCR ‚Üí ArgoCD ‚Üí GKE Autopilot
    ‚Üì                        ‚Üì
Terraform ‚Üí GCP Resources   Vault ‚Üí External Secrets ‚Üí K8s Secrets
    ‚Üì                        ‚Üì
Monitoring ‚Üê Prometheus ‚Üê Applications ‚Üí Logs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;production-grade-components&quot;&gt;&lt;strong&gt;Production-Grade Components&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;GitOps with ArgoCD&lt;/strong&gt;: Declarative deployments with dev/staging/prod environment promotion. Changes flow through Git, not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl apply&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Infrastructure as Code&lt;/strong&gt;: Terraform manages GKE clusters, static IPs, DNS records, and IAM policies.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Secrets Management&lt;/strong&gt;: HashiCorp Vault with External Secrets Operator. API keys and database credentials never touch Git or container images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-Environment Overlays&lt;/strong&gt;: Kustomize overlays handle environment-specific configurations. Production can use different resource limits, replica counts, and ingress rules than development.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Observability Stack&lt;/strong&gt;: Prometheus metrics collection with Grafana dashboards. Custom application metrics track API response times and cache hit rates.&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned&quot;&gt;What I Learned&lt;/h2&gt;

&lt;h3 id=&quot;1-mental-model-clarity&quot;&gt;&lt;strong&gt;1. Mental Model Clarity&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Complex infrastructure demands clear mental models. When ArgoCD shows ‚ÄúOutOfSync‚Äù status, I need to immediately understand:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Which Git commit should be deployed?&lt;/li&gt;
  &lt;li&gt;What Kustomize transformations apply?&lt;/li&gt;
  &lt;li&gt;How do external secrets affect pod startup?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Practical Impact&lt;/strong&gt;: Reduced debugging time from hours to minutes by maintaining clear architectural diagrams and understanding component dependencies.&lt;/p&gt;

&lt;h3 id=&quot;2-architectural-trade-offs&quot;&gt;&lt;strong&gt;2. Architectural Trade-offs&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Every decision has implications. For example: &lt;a href=&quot;/blog/vault-gitops/&quot;&gt;Vault as ArgoCD Application&lt;/a&gt; explores why I kept secrets management outside GitOps workflows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key Trade-off&lt;/strong&gt;: GitOps consistency vs. bootstrap complexity. Pure GitOps purists would manage Vault through ArgoCD, but circular dependencies make this operationally complex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decision Framework&lt;/strong&gt;: Evaluate based on operational simplicity, recovery scenarios, and production patterns, not theoretical purity.&lt;/p&gt;

&lt;h3 id=&quot;3-systematic-debugging&quot;&gt;&lt;strong&gt;3. Systematic Debugging&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Production systems fail in creative ways. In &lt;a href=&quot;/blog/debugging-502-errors/&quot;&gt;The Cluster Doctor&lt;/a&gt; blog post I demonstrate my approach to systematic troubleshooting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Core Principle&lt;/strong&gt;: Most failures are configuration mismatches, not infrastructure failures. Port conflicts, service discovery issues, and protocol mismatches accounted for the vast majority of my deployment problems.&lt;/p&gt;

&lt;h2 id=&quot;why-this-approach-works&quot;&gt;Why This Approach Works&lt;/h2&gt;

&lt;h3 id=&quot;for-learning&quot;&gt;&lt;strong&gt;For Learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Starting with a simple application let me focus on infrastructure concepts without application complexity interference. I could experiment with GitOps workflows, monitoring configurations, and security policies without worrying about business logic bugs. For the most part :)&lt;/p&gt;

&lt;h3 id=&quot;for-demonstrating&quot;&gt;&lt;strong&gt;For Demonstrating&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Systems Thinking&lt;/strong&gt;: Understanding how components interact across multiple layers (application, orchestration, infrastructure, security).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Production Readiness&lt;/strong&gt;: Moving beyond ‚Äúit works on my machine‚Äù to ‚Äúit works reliably in production with proper monitoring, security, and deployment practices.‚Äù&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Technology Integration&lt;/strong&gt;: Connecting multiple tools (ArgoCD, Vault, Prometheus, Terraform) into cohesive workflows rather than using them in isolation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decision Making&lt;/strong&gt;: Choosing appropriate architectural patterns based on trade-offs. A lot more fun, and difficult, than following tutorials.&lt;/p&gt;

&lt;h2 id=&quot;current-status&quot;&gt;Current Status&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://tv-hub.navillasa.dev&quot;&gt;Live application&lt;/a&gt; with full GitOps deployment, multi-environment promotion, and comprehensive monitoring.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next Phases&lt;/strong&gt;: Cost optimization dashboard, Redis caching layer, and advanced alerting rules.&lt;/p&gt;

&lt;h2 id=&quot;the-meta-learning&quot;&gt;The Meta-Learning&lt;/h2&gt;

&lt;p&gt;This project demonstrates that I can build production-ready infrastructure, make informed architectural decisions, and systematically debug complex systems.&lt;/p&gt;

&lt;p&gt;More than learning how to use individual tools, I learned how they integrate, &lt;em&gt;when&lt;/em&gt; to use them, and what trade-offs each decision creates.&lt;/p&gt;

&lt;p&gt;Building infrastructure for a simple application taught me more about production DevOps than building a complex application with simple deployment ever could.&lt;/p&gt;</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="architecture" /><category term="kubernetes" /><category term="argocd" /><category term="terraform" /><category term="gitops" /><category term="gke" /><category term="portfolio" /><summary type="html">The Paradox of Portfolio Projects</summary></entry><entry><title type="html">The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s</title><link href="http://localhost:4000/blog/debugging-502-errors/" rel="alternate" type="text/html" title="The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/debugging-502-errors</id><content type="html" xml:base="http://localhost:4000/blog/debugging-502-errors/">## The Problem Pattern

Throughout this project, I've hit the same type of issue repeatedly:
- **Frontend** looking for `dev-backend-service` instead of `prod-backend-service`
- **ArgoCD ingress** using port 443 instead of port 80
- **Vault** connection errors due to service discovery issues
- **External Secrets** pointing to wrong Vault paths

**The common thread**: 502 Bad Gateway errors caused by communication mismatches.

## The 502 Error: What It Really Means

```
502 Bad Gateway = &quot;I can reach something, but it's not responding correctly&quot;
```

**This is different from:**
- **404**: &quot;I can't find anything at this path&quot;
- **503**: &quot;Service is temporarily unavailable&quot;  
- **Connection timeout**: &quot;I can't reach anything at all&quot;

**502 specifically means**: The proxy/load balancer reached a backend, but the backend response was invalid.

## The Systematic Debugging Approach

### Step 1: Identify the Communication Chain

Every 502 has a path like this:
```
Client ‚Üí Load Balancer ‚Üí Service ‚Üí Pod
```

Find where it breaks by checking each link.

### Step 2: Check Backend Health in Load Balancer

```bash
# Get detailed ingress status
kubectl describe ingress &lt;ingress-name&gt; -n &lt;namespace&gt;

# Look for backend health status
# Example output:
ingress.kubernetes.io/backends: {
  &quot;k8s1-backend-service-443&quot;:&quot;UNHEALTHY&quot;  # ‚Üê The smoking gun!
}
```

**Key indicators:**
- `UNHEALTHY` = Backend isn't responding on expected port/protocol
- `HEALTHY` = Backend is fine, look elsewhere

## Pro Tips for Faster Debugging

### 1. **Use Port-Forward for Direct Testing**
```bash
# Bypass ingress/service and test pod directly
kubectl port-forward pod/&lt;pod-name&gt; 8080:8080
curl http://localhost:8080/health
```

### 2. **Check Multiple Namespaces**
```bash
# Service in wrong namespace is super common
kubectl get services --all-namespaces | grep &lt;service-name&gt;
```

The key insight: Most 502 errors are configuration mismatches, not infrastructure failures. A systematic approach beats random troubleshooting every time.</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="debugging" /><category term="devops" /><category term="kubernetes" /><category term="502-errors" /><category term="troubleshooting" /><category term="nginx" /><category term="ingress" /><summary type="html">The Problem Pattern</summary></entry><entry><title type="html">Why I Didn‚Äôt Make Vault an ArgoCD Application</title><link href="http://localhost:4000/blog/vault-gitops/" rel="alternate" type="text/html" title="Why I Didn‚Äôt Make Vault an ArgoCD Application" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/vault-gitops</id><content type="html" xml:base="http://localhost:4000/blog/vault-gitops/">## The Question

During the GitOps implementation, I considered whether HashiCorp Vault should be managed as an ArgoCD application alongside the other workloads. This post explains why I chose to keep Vault outside of the GitOps workflow.

## The Temptation

At first glance, managing Vault through ArgoCD seems _pretty_ cool:

- **Consistency**: Everything else is managed by ArgoCD
- **Version Control**: Vault configuration tracked in Git
- **Declarative**: Infrastructure as Code principles
- **Automation**: Automated deployments and updates

## The Bootstrap Problem

The fundamental issue is a **circular dependency**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    needs    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    needs    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ArgoCD    ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ External Secrets ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Vault   ‚îÇ
‚îÇ             ‚îÇ             ‚îÇ    Operator      ‚îÇ             ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚ñ≤                                                            ‚îÇ
       ‚îÇ                    manages (if GitOps)                     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**The problem**: If ArgoCD manages Vault, then ArgoCD depends on Vault (for secrets) which depends on ArgoCD (for deployment). This creates an unresolvable bootstrap dependency.

## Infrastructure Layers

I solved this by establishing clear **infrastructure layers**:

### Layer 0: Foundation Infrastructure
- **GKE Cluster** (Terraform)
- **Static IPs** (Terraform)
- **Vault** (kubectl apply)
- **External Secrets Operator** (Helm)

### Layer 1: GitOps Platform
- **ArgoCD** (kubectl apply)
- **ClusterSecretStore** (ArgoCD)

### Layer 2: Applications
- **tv-dashboard-dev** (ArgoCD)
- **tv-dashboard-prod** (ArgoCD)
- **monitoring-stack** (ArgoCD)

## Alternative Approaches Considered

### 1. Manual Sync Only
```yaml
# vault-app.yaml
spec:
  syncPolicy:
    automated: null  # No auto-sync to avoid bootstrap issues
```

**Pros**: Vault config in Git  
**Cons**: Manual intervention required, defeats GitOps automation

### 2. App-of-Apps Pattern
```yaml
# bootstrap-app.yaml - deployed manually
# Manages vault-app.yaml and argocd-apps.yaml
```

**Pros**: Complete GitOps coverage  
**Cons**: Complex bootstrap sequence, harder to debug

### 3. External Vault
Use a managed secret service (Google Secret Manager, AWS Secrets Manager)

**Pros**: No bootstrap problem, less operational overhead  
**Cons**: Vendor lock-in, less learning value for this project

## The Decision: Keep Vault Outside GitOps

**Reasons**:

1. **Operational Simplicity**: Vault is foundational infrastructure that should be stable and simple to manage
2. **Recovery Scenarios**: If ArgoCD fails, we can still access Vault to recover secrets
3. **Bootstrap Clarity**: Clear separation between foundation and application layers
4. **Production Patterns**: Many organizations treat secret management as Layer 0 infrastructure

## Production Considerations

In production environments, you might see:

### At Large Organizations
- **Dedicated Vault clusters** managed by platform teams
- **Vault-as-a-Service** provided to application teams
- **Manual deployment** with infrastructure automation (Terraform)

### At Cloud-Native Shops
- **Managed secret services** (AWS Secrets Manager, etc.)
- **External Secrets Operator** connecting to cloud providers
- **No self-hosted Vault** at all

### In GitOps Purist Repos
- **Everything in Git** including Vault
- **Complex bootstrap procedures** with operator dependencies
- **Acceptance of operational complexity** for consistency

## Key Takeaway

**Not everything needs to be in GitOps.** The right architectural boundary depends on:

- **Operational complexity** vs **consistency benefits**
- **Bootstrap dependencies** and **recovery scenarios**  
- **Team structure** and **operational expertise**
- **Compliance requirements** and **audit trails**

For my TV Dashboard project, keeping Vault as foundational infrastructure provided the right balance of simplicity and functionality while still demonstrating modern secret management practices.

## What I Learned

1. **Architectural boundaries matter** - not every tool fits every pattern
2. **Bootstrap dependencies** are real constraints in system design
3. **Operational simplicity** often trumps theoretical consistency
4. **Production patterns** should inform learning project decisions

The goal isn't to GitOps-ify everything ‚Äî it's to build systems that are **reliable, maintainable, and appropriate for their context**.

---

*This decision reflects my specific context and learning goals. Your environment may have different constraints and requirements.*</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="design" /><summary type="html">The Question</summary></entry></feed>