<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-19T14:44:11-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Natalie Villasana</title><subtitle>DevOps Engineer | navillasa.dev</subtitle><author><name>Natalie Villasana</name><email>navillasa.dev@gmail.com</email></author><entry><title type="html">The CI/CD Pipeline I Wish I’d Built Sooner</title><link href="http://localhost:4000/blog/gitops-cicd-pipeline/" rel="alternate" type="text/html" title="The CI/CD Pipeline I Wish I’d Built Sooner" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/gitops-cicd-pipeline</id><content type="html" xml:base="http://localhost:4000/blog/gitops-cicd-pipeline/">&lt;p&gt;When I started building my &lt;a href=&quot;/blog/tv-hub-project-overview/&quot;&gt;TV Hub project&lt;/a&gt;, I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here’s the GitOps pipeline I wish I’d built from day one.&lt;/p&gt;

&lt;h2 id=&quot;the-problem&quot;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;My original setup was very simple and anxiety-inducingly fragile. Every git push triggered a full deployment to production. No gates, no testing environment, just vibes.&lt;/p&gt;

&lt;p&gt;When a color-changing button broke the frontend one too many times, I realized I needed a system that could move fast without breaking things. The solution needed to automatically deploy tested code while preventing untested changes from reaching “production.”&lt;/p&gt;

&lt;h2 id=&quot;the-solution-two-speed-gitops&quot;&gt;The Solution: Two-Speed GitOps&lt;/h2&gt;

&lt;p&gt;I ended up with a hybrid approach that treats development and production very differently:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git push → GitHub Actions → new images → dev deploys automatically
                                      → prod requires manual promotion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here’s how it works:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Development environment:&lt;/strong&gt; Every successful build auto-deploys. Fast feedback, immediate testing, maximum iteration speed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Production environment:&lt;/strong&gt; Requires explicit promotion of a tested version. Slower, deliberate, with human accountability.&lt;/p&gt;

&lt;h2 id=&quot;how-the-pipeline-works&quot;&gt;How the Pipeline Works&lt;/h2&gt;

&lt;p&gt;When I push code to main, GitHub Actions runs the full test suite, builds Docker images, and tags them with a date-based version like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v20250819-f3b8541&lt;/code&gt;. If tests pass, it automatically updates the development Kubernetes manifests and pushes the change back to git.&lt;/p&gt;

&lt;p&gt;ArgoCD watches the git repo and syncs any changes to the development cluster within about 30 seconds. I can immediately test the new version at the dev URL.&lt;/p&gt;

&lt;p&gt;For production, there’s no automatic deployment. Instead, I run a promotion script:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# See what versions are available&lt;/span&gt;
./scripts/promote-to-prod.sh

&lt;span class=&quot;c&quot;&gt;# Promote a tested version to production  &lt;/span&gt;
./scripts/promote-to-prod.sh v20250819-f3b8541
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The script shows me exactly what will change, asks for confirmation, then updates the production manifests and pushes to git. ArgoCD syncs the change to production.&lt;/p&gt;

&lt;h2 id=&quot;technical-choices&quot;&gt;Technical Choices&lt;/h2&gt;

&lt;p&gt;I made several specific decisions that shaped how this pipeline works:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Date-based versioning:&lt;/strong&gt; Instead of incrementing numbers, I use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v$(date +%Y%m%d)-$(git-hash)&lt;/code&gt;. This makes it obvious when a version was created and provides a direct link back to the source code. Much easier for operations than trying to remember what v1.47 contained.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kustomize overlays:&lt;/strong&gt; The same base Kubernetes manifests work for both environments, with environment-specific patches. Development gets relaxed resource limits and debug logging; production gets proper resource constraints, and theoretically should get structured logs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ArgoCD split configuration:&lt;/strong&gt; The development ArgoCD app has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;automated: true&lt;/code&gt; for immediate deployments. The production app has automated sync disabled, requiring manual intervention for every change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Container registry strategy:&lt;/strong&gt; Both environments pull from the same registry, just with different tags. This ensures what I test in dev is exactly what deploys to prod—no separate build processes or subtle differences.&lt;/p&gt;

&lt;h2 id=&quot;what-i-could-have-done-differently&quot;&gt;What I Could Have Done Differently&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Full automation:&lt;/strong&gt; I could have set up comprehensive integration tests and deployed to production automatically if everything passes. The tooling exists—Argo Rollouts for progressive delivery, sophisticated test suites, automated rollback triggers.&lt;/p&gt;

&lt;p&gt;I chose manual promotion instead because I wanted deliberate production changes. This is my portfolio project after all! The slight overhead of manual promotion forces me to actually test changes in dev and think about the impact.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Branch-based environments:&lt;/strong&gt; Many teams use feature branches for feature environments, develop branch for staging, and main branch for production. I went with a simpler model where main always goes to dev, and production gets explicit promotions.&lt;/p&gt;

&lt;p&gt;The branch approach would have been more “textbook correct” but would have required more operational overhead managing multiple environments and branch policies. For a portfolio project, the current approach demonstrates the core GitOps concepts without unnecessary complexity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;External tools:&lt;/strong&gt; I could have used Flux instead of ArgoCD, or Tekton instead of GitHub Actions, or Jenkins X for the whole pipeline. The GitHub Actions + ArgoCD combination is incredibly common in the industry, well-documented, and has good operational tooling.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The manual promotion gate adds maybe 2 minutes to the deployment process but provides huge peace of mind. I can deploy more frequently because I’m confident each change has been tested and validated.&lt;/p&gt;

&lt;p&gt;For a team environment, I’d probably add Slack notifications for deployments and automated integration tests against the dev environment. But for a solo project, this pipeline hits the sweet spot of safety, speed, and simplicity.&lt;/p&gt;

&lt;p&gt;The beauty of GitOps is that these patterns scale. The same basic structure works whether you’re promoting manually like I do, or using sophisticated automated promotion with comprehensive test coverage. The deployment mechanism stays consistent—only the automation boundaries change.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This pipeline powers the &lt;a href=&quot;http://tv-hub.navillasa.dev&quot;&gt;TV Hub project&lt;/a&gt; with full source available in the &lt;a href=&quot;https://github.com/navillasa/tv-dashboard-k8s&quot;&gt;GitHub repo&lt;/a&gt;. You can see real deployment metrics in the &lt;a href=&quot;https://monitoring.navillasa.dev/d/e0c978bd-6077-403e-ab3b-ba03f4b34962/tv-hub-business-intelligence-dashboard&quot;&gt;live monitoring dashboard&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="gitops" /><category term="ci-cd" /><summary type="html">When I started building my TV Hub project, I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here’s the GitOps pipeline I wish I’d built from day one.</summary></entry><entry><title type="html">Building Production-Grade Infrastructure for a Simple App</title><link href="http://localhost:4000/blog/tv-hub-project-overview/" rel="alternate" type="text/html" title="Building Production-Grade Infrastructure for a Simple App" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/tv-hub-project-overview</id><content type="html" xml:base="http://localhost:4000/blog/tv-hub-project-overview/">## The Paradox of Portfolio Projects

I faced a classic engineering dilemma: **build something complex that needs sophisticated infrastructure, or build something simple and add sophisticated infrastructure anyway?**

After cycling through ideas (distributed microservices, ML pipelines, IoT dashboards) I kept returning to one idea: a TV show aggregator. Because it was an app with some appeal and usefulness beyond demonstrating its own tech stack. The app itself is straightforward: fetch trending shows from TMDB and TVmaze APIs, rank by popularity, display in a clean interface.

**But there's a twist.** I deployed it with enterprise-grade DevOps practices that would typically serve a team of a 50+ engineers. Or at least a dozen.

## Strategic Design Decisions

### **Simple App, Complex Infrastructure**

I specifically chose a simple application to isolate and focus on infrastructure complexity. Here's why this approach worked:

**Cognitive Load Management**: Managing ArgoCD, Vault integration, and Prometheus monitoring is challenging enough without debugging distributed system race conditions simultaneously.

**Clear Separation of Concerns**: When something breaks, I can immediately identify whether it's an application bug (less likely, given the simplicity) or an infrastructure configuration issue (very likely, given the learning curve).

**Real-World Simulation**: Most production systems start simple and grow complex. This project demonstrates the infrastructure foundation that scales.

## Technical Architecture

The application architecture is intentionally minimal:

```
React Frontend ←→ Node.js API ←→ PostgreSQL ←→ External APIs
```

The **infrastructure architecture** tells a different story:

```
GitHub → Actions → GCR → ArgoCD → GKE Autopilot
    ↓                        ↓
Terraform → GCP Resources   Vault → External Secrets → K8s Secrets
    ↓                        ↓
Monitoring ← Prometheus ← Applications → Logs
```

### **Production-Grade Components**

**GitOps with ArgoCD**: Declarative deployments with dev/staging/prod environment promotion. Changes flow through Git, not `kubectl apply`.

**Infrastructure as Code**: Terraform manages GKE clusters, static IPs, DNS records, and IAM policies.

**Secrets Management**: HashiCorp Vault with External Secrets Operator. API keys and database credentials never touch Git or container images.

**Multi-Environment Overlays**: Kustomize overlays handle environment-specific configurations. Production can use different resource limits, replica counts, and ingress rules than development.

**Observability Stack**: Prometheus metrics collection with Grafana dashboards. Custom application metrics track API response times and cache hit rates.

## What I Learned

### **1. Mental Model Clarity**

Complex infrastructure demands clear mental models. When ArgoCD shows &quot;OutOfSync&quot; status, I need to immediately understand:
- Which Git commit should be deployed?
- What Kustomize transformations apply?
- How do external secrets affect pod startup?

**Practical Impact**: Reduced debugging time from hours to minutes by maintaining clear architectural diagrams and understanding component dependencies.

### **2. Architectural Trade-offs**

Every decision has implications. For example: [Vault as ArgoCD Application]({{ '/blog/vault-gitops/' | relative_url }}) explores why I kept secrets management outside GitOps workflows.

**Key Trade-off**: GitOps consistency vs. bootstrap complexity. Pure GitOps purists would manage Vault through ArgoCD, but circular dependencies make this operationally complex.

**Decision Framework**: Evaluate based on operational simplicity, recovery scenarios, and production patterns, not theoretical purity.

### **3. Systematic Debugging**

Production systems fail in creative ways. In [The Cluster Doctor]({{ '/blog/debugging-502-errors/' | relative_url }}) blog post I demonstrate my approach to systematic troubleshooting.

**Core Principle**: Most failures are configuration mismatches, not infrastructure failures. Port conflicts, service discovery issues, and protocol mismatches accounted for the vast majority of my deployment problems.

## Why This Approach Works

### **For Learning**
Starting with a simple application let me focus on infrastructure concepts without application complexity interference. I could experiment with GitOps workflows, monitoring configurations, and security policies without worrying about business logic bugs. For the most part :)

### **For Demonstrating**
**Systems Thinking**: Understanding how components interact across multiple layers (application, orchestration, infrastructure, security).

**Production Readiness**: Moving beyond &quot;it works on my machine&quot; to &quot;it works reliably in production with proper monitoring, security, and deployment practices.&quot;

**Technology Integration**: Connecting multiple tools (ArgoCD, Vault, Prometheus, Terraform) into cohesive workflows rather than using them in isolation.

**Decision Making**: Choosing appropriate architectural patterns based on trade-offs. A lot more fun, and difficult, than following tutorials.

## Current Status
[Live application](http://tv-hub.navillasa.dev) with full GitOps deployment, multi-environment promotion, and comprehensive monitoring.

**Next Phases**: Cost optimization dashboard, Redis caching layer, and advanced alerting rules.

## The Meta-Learning

This project demonstrates that I can build production-ready infrastructure, make informed architectural decisions, and systematically debug complex systems.

More than learning how to use individual tools, I learned how they integrate, _when_ to use them, and what trade-offs each decision creates.

Building infrastructure for a simple application taught me more about production DevOps than building a complex application with simple deployment ever could.</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="architecture" /><category term="kubernetes" /><category term="argocd" /><category term="terraform" /><category term="gitops" /><category term="gke" /><category term="portfolio" /><summary type="html">The Paradox of Portfolio Projects</summary></entry><entry><title type="html">The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s</title><link href="http://localhost:4000/blog/debugging-502-errors/" rel="alternate" type="text/html" title="The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/debugging-502-errors</id><content type="html" xml:base="http://localhost:4000/blog/debugging-502-errors/">## The Problem Pattern

Throughout this project, I've hit the same type of issue repeatedly:
- **Frontend** looking for `dev-backend-service` instead of `prod-backend-service`
- **ArgoCD ingress** using port 443 instead of port 80
- **Vault** connection errors due to service discovery issues
- **External Secrets** pointing to wrong Vault paths

**The common thread**: 502 Bad Gateway errors caused by communication mismatches.

## The 502 Error: What It Really Means

```
502 Bad Gateway = &quot;I can reach something, but it's not responding correctly&quot;
```

**This is different from:**
- **404**: &quot;I can't find anything at this path&quot;
- **503**: &quot;Service is temporarily unavailable&quot;  
- **Connection timeout**: &quot;I can't reach anything at all&quot;

**502 specifically means**: The proxy/load balancer reached a backend, but the backend response was invalid.

## The Systematic Debugging Approach

### Step 1: Identify the Communication Chain

Every 502 has a path like this:
```
Client → Load Balancer → Service → Pod
```

Find where it breaks by checking each link.

### Step 2: Check Backend Health in Load Balancer

```bash
# Get detailed ingress status
kubectl describe ingress &lt;ingress-name&gt; -n &lt;namespace&gt;

# Look for backend health status
# Example output:
ingress.kubernetes.io/backends: {
  &quot;k8s1-backend-service-443&quot;:&quot;UNHEALTHY&quot;  # ← The smoking gun!
}
```

**Key indicators:**
- `UNHEALTHY` = Backend isn't responding on expected port/protocol
- `HEALTHY` = Backend is fine, look elsewhere

## Pro Tips for Faster Debugging

### 1. **Use Port-Forward for Direct Testing**
```bash
# Bypass ingress/service and test pod directly
kubectl port-forward pod/&lt;pod-name&gt; 8080:8080
curl http://localhost:8080/health
```

### 2. **Check Multiple Namespaces**
```bash
# Service in wrong namespace is super common
kubectl get services --all-namespaces | grep &lt;service-name&gt;
```

The key insight: Most 502 errors are configuration mismatches, not infrastructure failures. A systematic approach beats random troubleshooting every time.</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="debugging" /><category term="devops" /><category term="kubernetes" /><category term="502-errors" /><category term="troubleshooting" /><category term="nginx" /><category term="ingress" /><summary type="html">The Problem Pattern</summary></entry><entry><title type="html">Why I Didn’t Make Vault an ArgoCD Application</title><link href="http://localhost:4000/blog/vault-gitops/" rel="alternate" type="text/html" title="Why I Didn’t Make Vault an ArgoCD Application" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/vault-gitops</id><content type="html" xml:base="http://localhost:4000/blog/vault-gitops/">## The Question

During the GitOps implementation, I considered whether HashiCorp Vault should be managed as an ArgoCD application alongside the other workloads. This post explains why I chose to keep Vault outside of the GitOps workflow.

## The Temptation

At first glance, managing Vault through ArgoCD seems _pretty_ cool:

- **Consistency**: Everything else is managed by ArgoCD
- **Version Control**: Vault configuration tracked in Git
- **Declarative**: Infrastructure as Code principles
- **Automation**: Automated deployments and updates

## The Bootstrap Problem

The fundamental issue is a **circular dependency**:

```
┌─────────────┐    needs    ┌──────────────────┐    needs    ┌───────────┐
│   ArgoCD    │ ──────────► │ External Secrets │ ──────────► │   Vault   │
│             │             │    Operator      │             │           │
└─────────────┘             └──────────────────┘             └───────────┘
       ▲                                                            │
       │                    manages (if GitOps)                     │
       └────────────────────────────────────────────────────────────┘
```

**The problem**: If ArgoCD manages Vault, then ArgoCD depends on Vault (for secrets) which depends on ArgoCD (for deployment). This creates an unresolvable bootstrap dependency.

## Infrastructure Layers

I solved this by establishing clear **infrastructure layers**:

### Layer 0: Foundation Infrastructure
- **GKE Cluster** (Terraform)
- **Static IPs** (Terraform)
- **Vault** (kubectl apply)
- **External Secrets Operator** (Helm)

### Layer 1: GitOps Platform
- **ArgoCD** (kubectl apply)
- **ClusterSecretStore** (ArgoCD)

### Layer 2: Applications
- **tv-dashboard-dev** (ArgoCD)
- **tv-dashboard-prod** (ArgoCD)
- **monitoring-stack** (ArgoCD)

## Alternative Approaches Considered

### 1. Manual Sync Only
```yaml
# vault-app.yaml
spec:
  syncPolicy:
    automated: null  # No auto-sync to avoid bootstrap issues
```

**Pros**: Vault config in Git  
**Cons**: Manual intervention required, defeats GitOps automation

### 2. App-of-Apps Pattern
```yaml
# bootstrap-app.yaml - deployed manually
# Manages vault-app.yaml and argocd-apps.yaml
```

**Pros**: Complete GitOps coverage  
**Cons**: Complex bootstrap sequence, harder to debug

### 3. External Vault
Use a managed secret service (Google Secret Manager, AWS Secrets Manager)

**Pros**: No bootstrap problem, less operational overhead  
**Cons**: Vendor lock-in, less learning value for this project

## The Decision: Keep Vault Outside GitOps

**Reasons**:

1. **Operational Simplicity**: Vault is foundational infrastructure that should be stable and simple to manage
2. **Recovery Scenarios**: If ArgoCD fails, we can still access Vault to recover secrets
3. **Bootstrap Clarity**: Clear separation between foundation and application layers
4. **Production Patterns**: Many organizations treat secret management as Layer 0 infrastructure

## Production Considerations

In production environments, you might see:

### At Large Organizations
- **Dedicated Vault clusters** managed by platform teams
- **Vault-as-a-Service** provided to application teams
- **Manual deployment** with infrastructure automation (Terraform)

### At Cloud-Native Shops
- **Managed secret services** (AWS Secrets Manager, etc.)
- **External Secrets Operator** connecting to cloud providers
- **No self-hosted Vault** at all

### In GitOps Purist Repos
- **Everything in Git** including Vault
- **Complex bootstrap procedures** with operator dependencies
- **Acceptance of operational complexity** for consistency

## Key Takeaway

**Not everything needs to be in GitOps.** The right architectural boundary depends on:

- **Operational complexity** vs **consistency benefits**
- **Bootstrap dependencies** and **recovery scenarios**  
- **Team structure** and **operational expertise**
- **Compliance requirements** and **audit trails**

For my TV Dashboard project, keeping Vault as foundational infrastructure provided the right balance of simplicity and functionality while still demonstrating modern secret management practices.

## What I Learned

1. **Architectural boundaries matter** - not every tool fits every pattern
2. **Bootstrap dependencies** are real constraints in system design
3. **Operational simplicity** often trumps theoretical consistency
4. **Production patterns** should inform learning project decisions

The goal isn't to GitOps-ify everything — it's to build systems that are **reliable, maintainable, and appropriate for their context**.

---

*This decision reflects my specific context and learning goals. Your environment may have different constraints and requirements.*</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="design" /><summary type="html">The Question</summary></entry></feed>