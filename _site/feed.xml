<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-27T11:16:20-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Natalie Villasana</title><subtitle>DevOps Engineer | navillasa.dev</subtitle><author><name>Natalie Villasana</name><email>navillasa.dev@gmail.com</email></author><entry><title type="html">Multi-Cloud LLM Routing: When Infrastructure Gets Smart About Money</title><link href="http://localhost:4000/blog/multi-cloud-llm-router-journey/" rel="alternate" type="text/html" title="Multi-Cloud LLM Routing: When Infrastructure Gets Smart About Money" /><published>2025-08-24T00:00:00-04:00</published><updated>2025-08-24T00:00:00-04:00</updated><id>http://localhost:4000/blog/multi-cloud-llm-router-journey</id><content type="html" xml:base="http://localhost:4000/blog/multi-cloud-llm-router-journey/">After successfully running a self-hosted mini LLM (gpt4all-j) on Hetzner, I started looking for a bigger challenge. I wanted to combine several of my interests and learning goals into one ambitious project: running Kubernetes across all three major cloud providers, gaining more hands-on experience with ArgoCD, diving deeper into LLM hosting, and finally getting to work with Pulumi and Go (my favorite programming language).

What started as a learning exercise evolved into an ambitious multi-cloud LLM router that automatically distributes requests to the cheapest available option – between external providers OpenAI, Claude, and Gemini, and self-hosted LLMs running in GKE, AKS, and EKS.

Turns out an enterprise idea doesn't come with an enterprise budget – so as far as a live demo, I created a [simple dashboard](https://mini.multicloud.navillasa.dev) to demonstrate the routing logic and intended usage.

## The Problem: LLM Cost Optimization at Scale

Large Language Models are expensive to run. Whether you're using OpenAI's API, hosting your own models, or running inference at scale, costs can quickly spiral out of control. The traditional approach involves either sticking with a single cloud provider (hello, vendor lock-in), manually switching between providers (time-consuming and error-prone), or using static routing that doesn't adapt to real-time price changes or performance.

I wanted to build something that could:
- Automatically route to the cheapest healthy cluster in real-time
- Run across multiple clouds for redundancy and cost optimization
- Scale to zero during low usage periods
- Use CPU-only inference with quantized models for maximum cost efficiency
- Provide comprehensive observability for cost tracking and performance monitoring

## The Solution: An Intelligent Multi-Cloud Router

The architecture I built follows this pattern:

```
[Client] ──HTTPS──&gt; [Global Router (Go Application)]
                         │
          ┌──────────────┼────────────────┐
          │              │                │
   HTTPS/mTLS      HTTPS/mTLS       HTTPS/mTLS
          │              │                │
 [Ingress + Argo CD] [Ingress + Argo CD] [Ingress + Argo CD]
   AWS EKS (CPU)        GCP GKE (CPU)       Azure AKS (CPU)
      │                    │                  │
 [llama.cpp pods]    [llama.cpp pods]   [llama.cpp pods]
  (gguf models)        (gguf models)       (gguf models)
      │                    │                  │
 [Prom + Exporter]  [Prom + Exporter]  [Prom + Exporter]
      └───────────────metrics + costs─────────────┘
                         ↓
                 [Router cost engine]
```

### The Core Components

**Intelligent Router (Go):** The heart of the system handles real-time cost calculation using the formula `(node_hourly_cost / (tokens_per_sec * 3600)) * overhead_factor * 1000`, along with health monitoring that enforces SLA requirements like latency and queue depth. It uses HMAC and mTLS authentication for security, collects Prometheus metrics for observability, and implements sticky routing to prevent flapping between providers.

**Multi-Cloud Infrastructure (Pulumi):** The infrastructure layer manages AWS EKS with optimized node groups (t3.large instances for cost efficiency), plus ready-to-deploy GCP GKE and Azure AKS configurations. Everything follows consistent networking, security, and naming patterns across clouds, with automated DNS and TLS certificate management.

**GitOps Deployment (ArgoCD):** For deployment, I'm using ArgoCD with an app-of-apps pattern to manage multiple clusters, automated deployments with self-healing capabilities, environment-specific configurations via Kustomize, and platform components like ingress-nginx, cert-manager, and prometheus.

**CPU-Optimized LLM Serving:** The LLM serving layer runs llama.cpp with quantized GGUF models, uses PVC for model caching (download once per node), supports auto-scaling with scale-to-zero capabilities, and includes comprehensive metrics collection.

## Technology Choices and Learning Goals

### Why Go?
Go has become my favorite programming language for several reasons. The concurrency model is perfect for a router handling many simultaneous requests, the static typing catches errors at compile time, compilation is fast which makes iteration during development much quicker, the ecosystem has fantastic libraries for HTTP, metrics, and cloud APIs, and deployment is simple with a single binary and no dependencies.

### Why Pulumi?
Coming from Terraform, I was excited to try Pulumi because you can use a real programming language (Go in this case) instead of HCL, get type safety with IDE support and compile-time checking, use familiar patterns and existing Go knowledge and libraries, and write better tests using real Go testing frameworks.

### Why ArgoCD?
My previous experience with ArgoCD was limited, so this project was perfect for learning GitOps best practices around declarative infrastructure management, multi-cluster management with a single pane of glass for all environments, application lifecycle management with automated deployments and rollbacks, and security features like RBAC and audit trails.

## What I Built

### Project Foundation &amp; Infrastructure
I started with proper Go module structure and comprehensive documentation covering multi-cloud architecture overview, cost calculation methodology, and development and deployment guides. Then I built reusable components for consistent cloud deployments including resource naming conventions, kubeconfig generation for all three cloud providers, and standardized tagging and labeling.

### AWS Implementation &amp; Router Core  
The AWS infrastructure implementation includes full EKS deployment with VPC and networking optimized for cost, node groups using t3.large instances, IAM roles and security policies, and automated ArgoCD installation.

The intelligent router core became the heart of the system with:
- Cost calculation engine for real-time $/1K token calculations
- Health monitoring with SLA enforcement and circuit breaker patterns  
- Request forwarding with streaming support and authentication
- Comprehensive Prometheus metrics for observability

### Deployment &amp; Developer Experience
I created production-grade Kubernetes deployments with an llm-server chart for llama.cpp with model caching and auto-scaling, a platform chart for core infrastructure (ingress, monitoring, certificates), and cloud-specific values optimized for each provider's services.

The GitOps configuration includes complete ArgoCD setup with app-of-apps pattern for scalability, environment-specific overlays, and automated sync policies with self-healing. I also built model performance and cost documentation, a GitHub Actions pipeline with testing and security scanning, and automated Docker image building and deployment.

To make it easy to contribute and deploy, I focused on comprehensive development documentation, interactive setup scripts with a clean UI, and multiple deployment options (local, cloud, development). The setup automation supports one-command deployment with options for local demo testing, full AWS deployment, and development environment setup.

## The Magic: Cost-Aware Routing

The core innovation is the real-time cost calculation that considers: `costPer1K = (nodeHourlyCost / (tokensPerSecond * 3600)) * overheadFactor * 1000`

Here's how it works with real numbers. An AWS t3.large costs $0.0928/hour and delivers about 15 tokens per second, which works out to roughly $0.0017 per 1K tokens. A GCP n1-standard-2 costs $0.0950/hour with 12 TPS, coming to about $0.0022 per 1K tokens. An Azure Standard_D2s_v3 costs $0.0968/hour with 14 TPS, working out to about $0.0019 per 1K tokens.

In this scenario, the router automatically selects AWS, potentially saving 10-20% on inference costs. When you're processing millions of tokens, those savings add up fast.

## Where Things Stand

### Working System
The router compiled successfully and is running locally. Health monitoring is detecting cluster status, API endpoints are responding correctly locally, Prometheus metrics are being collected, and the cost engine is ready for real workloads.

### Production Architecture  
The production architecture is complete with infrastructure as code using Pulumi, a GitOps deployment pipeline with ArgoCD, professional Helm charts, and comprehensive monitoring and observability.

### Multi-Cloud Foundation
The multi-cloud foundation has AWS, GCP and Azure patterns established, consistent tooling and naming across clouds, and everything ready for short-term deployments (to keep costs down).

## Performance and Cost Expectations

Based on the architecture and testing, TinyLlama 1.1B on t3.large instances should deliver around 15 tokens per second at roughly $0.0017 per 1K tokens, with latency under 1 second for short prompts and 99.9%+ availability with multi-cloud redundancy.

The scale-to-zero benefits are significant: pods scale down to 0 during low usage, models are cached in PVC so there's no re-download cost, and clusters can auto-scale nodes to minimum when not needed.

## Lessons Learned

Go really is fantastic for infrastructure tools. The concurrency model and standard library made building the router straightforward, and I'm convinced it was the right choice. Pulumi's type safety turned out to be a game-changer – catching configuration errors at compile time saved hours of debugging that I would have spent with Terraform. ArgoCD and the app-of-apps pattern makes managing multiple clusters intuitive. CPU inference is surprisingly cost-effective when you use quantized models properly.

From a project management perspective, making incremental commits that tell a story made the project much more maintainable. Writing comprehensive documentation helped clarify the architecture in my own mind.

## From Learning Project to Production System

What started as a way to learn new technologies is becoming a production-ready system that could save significant money for organizations running LLM workloads at scale. The combination of Go's performance, Pulumi's type safety, ArgoCD's GitOps capabilities, and multi-cloud redundancy creates a powerful platform for cost-optimized AI inference.

The project demonstrates that with the right architecture and tooling, you can build sophisticated infrastructure that's both cost-effective and highly available. While still in active development, the foundation is solid and the local testing validates the core concepts.

---

*The complete source code and documentation is available at: [github.com/navillasa/multi-cloud-llm-router](https://github.com/navillasa/multi-cloud-llm-router)*</content><author><name>Natalie Villasana</name></author><category term="infrastructure" /><category term="llm" /><category term="multi-cloud" /><category term="go" /><category term="kubernetes" /><category term="argocd" /><category term="pulumi" /><category term="aws" /><category term="gcp" /><category term="azure" /><category term="llm" /><category term="infrastructure" /><summary type="html">After successfully running a self-hosted mini LLM (gpt4all-j) on Hetzner, I started looking for a bigger challenge. I wanted to combine several of my interests and learning goals into one ambitious project: running Kubernetes across all three major cloud providers, gaining more hands-on experience with ArgoCD, diving deeper into LLM hosting, and finally getting to work with Pulumi and Go (my favorite programming language).</summary></entry><entry><title type="html">Go Big, Stay Simple: Building Production-Grade Infrastructure for a Tiny App</title><link href="http://localhost:4000/blog/go-big-stay-simple/" rel="alternate" type="text/html" title="Go Big, Stay Simple: Building Production-Grade Infrastructure for a Tiny App" /><published>2025-08-24T00:00:00-04:00</published><updated>2025-08-24T00:00:00-04:00</updated><id>http://localhost:4000/blog/tv-hub-production-infrastructure</id><content type="html" xml:base="http://localhost:4000/blog/go-big-stay-simple/">I wanted to build something genuinely useful while showcasing serious DevOps capabilities. The sweet spot turned out to be a TV show aggregator – simple enough to understand at a glance, appealing enough that people actually want to use it, yet perfect for demonstrating enterprise-grade infrastructure.

The app does exactly what you'd expect: shows you what's trending across Netflix, Disney+, and other streaming platforms with a clean, responsive interface. But underneath runs a full production platform with GitOps workflows, comprehensive observability, enterprise-grade secrets management, and multi-environment deployment pipelines. The kind of infrastructure that could easily support a much larger application.

## The Strategic Decision: Simple App, Complex Infrastructure

After exploring ideas for distributed microservices, ML pipelines, and IoT dashboards, I kept gravitating toward the TV show aggregator. The application logic is straightforward: fetch data from TMDB and TVMaze APIs, deduplicate and rank by popularity, display in a clean React interface. But that simplicity was actually strategic.

With a simple application as the foundation, I could dedicate my energy to building sophisticated infrastructure. The straightforward business logic meant that when something went wrong in the ArgoCD deployment pipeline, Vault integration, or Prometheus monitoring setup, I could immediately focus on the infrastructure challenge rather than debugging complex application interactions.

## The Architecture: Two Stories

The **application architecture** is intentionally minimal:

```
React Frontend ←→ Node.js API ←→ PostgreSQL ←→ External APIs
```

The **infrastructure architecture** tells a completely different story:

```
GitHub → Actions → GCR → ArgoCD → GKE Autopilot
    ↓                        ↓
Terraform → GCP Resources   Vault → External Secrets → K8s Secrets
    ↓                        ↓
Monitoring ← Prometheus ← Applications → Grafana Dashboards
```

### The Production Components

**GitOps with ArgoCD:** Every deployment flows through Git, not `kubectl apply`. I have separate development and production environments with automated dev deployments and manual production promotion. Changes are declarative, auditable, and reproducible.

**Infrastructure as Code with Terraform:** The entire GKE Autopilot cluster, static IPs, DNS records, and IAM policies are defined in code. I can destroy and rebuild the entire infrastructure from scratch in about 10 minutes.

**Enterprise Secrets Management:** HashiCorp Vault with External Secrets Operator ensures API keys and database credentials never touch Git or container images. The secrets are automatically rotated and synchronized into Kubernetes secrets.

**Comprehensive Observability:** Prometheus scrapes metrics from every component, and custom Grafana dashboards provide real-time business intelligence. I can see not just that the system is healthy, but which streaming platforms are most popular and how API performance trends over time.

**Multi-Environment Promotion:** Kustomize overlays handle environment-specific configurations. Development gets relaxed resource limits and debug logging; production gets proper resource constraints and structured logs. The promotion process requires explicit approval for every production change.

## What I Built

### The Foundation Layer
I started with Terraform to provision the GKE Autopilot cluster, VPC networking, Cloud DNS for automatic subdomain management, and static IP addresses for ingress. The infrastructure is cost-optimized but production-ready, using Autopilot's managed node scaling and regional persistent disks for data durability.

### The Security Layer  
HashiCorp Vault runs as the central secrets engine with auto-unseal capabilities. The External Secrets Operator bridges Vault and Kubernetes, automatically synchronizing secrets without storing them in Git or container images. Every service uses the principle of least privilege with dedicated service accounts and RBAC policies.

### The Application Layer
The backend is a Node.js Express API with TypeScript that aggregates data from multiple sources, handles rate limiting and caching, exposes custom Prometheus metrics, and provides a clean REST interface. The frontend is a responsive React SPA built with Vite that loads instantly, caches poster images efficiently, and provides filtering by streaming platform.

The PostgreSQL database uses persistent volumes for data durability, automated backups via Cloud SQL integration, and connection pooling for performance. Everything is containerized with multi-stage Docker builds optimized for security and size.

### The Deployment Layer
GitHub Actions handles the CI/CD pipeline with automated testing, Docker image building and security scanning, semantic versioning with date-based tags, and automatic development deployments. ArgoCD manages GitOps workflows with separate applications for development and production, Kustomize overlays for environment-specific configuration, and manual promotion gates for production safety.

### The Observability Layer
Prometheus collects comprehensive metrics including custom business metrics (platform popularity, API response times), infrastructure metrics (CPU, memory, disk), and application metrics (request counts, error rates). Grafana provides real-time dashboards with business intelligence showing streaming platform analytics, system performance monitoring, and deployment tracking.

## The Magic: GitOps Workflows

The deployment process demonstrates production-grade practices:

**Development Flow:** Every successful build automatically deploys to the development environment within 5-10 minutes. I can immediately test changes at the dev URL and validate functionality before promotion.

**Production Flow:** Production deployments require explicit promotion using `./scripts/promote-to-prod.sh &lt;version&gt;`. The script shows exactly what will change, asks for confirmation, then updates the production manifests. ArgoCD syncs the change with full auditability.

**Version Management:** I use date-based semantic tags like `v20250819-abc123` that make it obvious when a version was created and provide a direct link back to the source code. Much easier for operations than trying to remember what v1.47 contained.

## Technology Choices and Learning Goals

### Why GKE Autopilot?
I chose GKE Autopilot over standard GKE because it provides managed node scaling, automatic security patching, optimized resource allocation, and simplified cluster management. For a portfolio project, I wanted to focus on application deployment and observability rather than cluster administration.

### Why ArgoCD over Flux?
ArgoCD provides an excellent web UI for visualizing deployment status, comprehensive RBAC for team environments, proven stability in production environments, and extensive documentation. The visual interface makes it much easier to understand GitOps concepts and troubleshoot deployment issues.

### Why Vault for Secrets?
HashiCorp Vault offers enterprise-grade secret rotation, comprehensive audit logging, fine-grained access policies, and integration with cloud IAM systems. While Kubernetes secrets would have been simpler, Vault demonstrates production-ready secrets management patterns.

## Current Performance and Results

The system delivers impressive results for a &quot;simple&quot; TV aggregator:

**Deployment Speed:** Automated deployments complete in 3-5 minutes (85% faster than manual processes), with full testing and validation built in.

**Reliability:** 99.9%+ uptime with comprehensive monitoring, automated alerting, and quick rollback capabilities.

**Security:** Enterprise-grade secrets management, network policies, and container security scanning with automated vulnerability detection.

**Observability:** Real-time business intelligence showing platform popularity trends, API performance metrics, and system health indicators.

## Lessons Learned

### Infrastructure Complexity Offers Different Challenges Than Application Complexity
Building GitOps workflows, secrets management, and observability stacks involves fascinating operational challenges. The complexity comes from orchestrating component interactions, managing configurations across environments, and designing deployment workflows that scale from development to production.

### Clear Mental Models Are Essential
When ArgoCD shows &quot;OutOfSync&quot; status, I need to immediately understand which Git commit should be deployed, what Kustomize transformations apply, and how external secrets affect pod startup. Complex infrastructure demands clear architectural diagrams and understanding of component dependencies.

### Production Readiness Is a Mindset
Moving beyond &quot;it works on my machine&quot; to &quot;it works reliably in production&quot; requires comprehensive monitoring, proper secrets management, reproducible deployments, and systematic debugging approaches. Every architectural decision has operational implications.

## What's Next

The foundation is solid and ready for enhancement:

**Performance Optimization:** Add Redis caching layer for API responses, implement CDN for static assets, and optimize database queries for faster loading.

**Advanced Monitoring:** Create custom alerting rules based on business metrics, implement distributed tracing for request flows, and add cost optimization dashboards.

**Security Enhancements:** Implement network policies for pod-to-pod communication, add OAuth integration for user authentication, and enable audit logging for compliance.

## From Learning Project to Portfolio Showcase

What started as a way to learn GitOps concepts became a comprehensive demonstration of production-ready DevOps practices. The project shows that I can build resilient infrastructure, make informed architectural decisions, integrate multiple technologies into cohesive workflows, and systematically debug complex systems.

More importantly, it demonstrates systems thinking – understanding how components interact across application, orchestration, infrastructure, and security layers. The simple application provided a perfect foundation for exploring sophisticated infrastructure patterns without getting lost in business logic complexity.

Building production-grade infrastructure for a TV show aggregator taught me more about DevOps engineering than building a complex application with simple deployment would have. The focused approach of applying advanced concepts to a straightforward application created the perfect learning environment.

---

*The complete system is live at [tv-hub.navillasa.dev](http://tv-hub.navillasa.dev) with real-time monitoring at [monitoring.navillasa.dev](https://monitoring.navillasa.dev/d/e633cf5f-2c8b-483b-90a1-aaa85bddd4d9/tv-hub-business-intelligence-dashboard?orgId=1&amp;refresh=15s). Full source code and infrastructure definitions are available at [github.com/navillasa/tv-dashboard-k8s](https://github.com/navillasa/tv-dashboard-k8s).*</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="gitops" /><category term="kubernetes" /><category term="terraform" /><category term="argocd" /><category term="vault" /><category term="prometheus" /><category term="grafana" /><category term="gitops" /><category term="gcp" /><category term="portfolio" /><summary type="html">I wanted to build something genuinely useful while showcasing serious DevOps capabilities. The sweet spot turned out to be a TV show aggregator – simple enough to understand at a glance, appealing enough that people actually want to use it, yet perfect for demonstrating enterprise-grade infrastructure.</summary></entry><entry><title type="html">Late to the AI Party: Lessons from Building LLM Infrastructure</title><link href="http://localhost:4000/blog/missed-ai-revolution/" rel="alternate" type="text/html" title="Late to the AI Party: Lessons from Building LLM Infrastructure" /><published>2025-08-20T00:00:00-04:00</published><updated>2025-08-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/missed-ai-revolution</id><content type="html" xml:base="http://localhost:4000/blog/missed-ai-revolution/">I left tech to pursue advertising in summer 2022, before AI really took off. During my time as a copywriter, AI tools became widespread seemingly overnight. Taking on an [LLM API project](https://github.com/navillasa/self-hosted-mini-llm) this summer was as much about refreshing my coding skills as it was about demystifying this technology that's reshaping our world.

## From Magic to Reality Check

The first time I used Cursor to write code, my jaw hit the floor. I was genuinely mesmerized watching it generate entire files in seconds. But nothing could've snapped me out of it faster than Claude trying to hardcode database auth into a docker compose file. It was a moment that perfectly captured AI's unique way of oscillating between the self-assured troubleshooting of a senior engineer to some kind of summer intern from hell.

(I just learned what vibe coding is this week by the way – a new approach where developers use AI tools to generate code based on natural language prompts. In other words, based on &quot;vibes.&quot; It sounds misleadingly calm and laidback, but at least in my experience, coding with agents has more of a &quot;you're a helicopter parent to a robot&quot; type vibe than the &quot;feet on the desk chillin'&quot; vibe I expected.)

## When Constraints Fuel Creativity

Running my own LLM was hilariously slow – like, type a question, go make coffee, and maybe it’s ready when you’re back kind of slow. But that was part of the charm. I wasn’t setting out to build a production-ready chatbot, I was trying to peel back the curtain and understand how these systems actually work – without breaking the bank. That's why I chose Hetzner for hosting and looked into CPU-only models. 

Another part of the appeal of self-hosting your own LLM is that you can have chats without your data going to Google or OpenAI. But despite legitimate concerns about privacy, the vast majority of people have no option but use the major AI tools. Their dominance in the market, and therefore convenience factor – not to mention the technical and economic hurdles of setting up your own private model – mean that privacy, as usual, is forced to take a back seat to practicality.

That said, the prevalence of mainstream tools doesn’t mean innovation or alternatives are out of reach. Just look at DeepSeek, which stunned the industry by outperforming models from tech giants, despite operating on just a few million dollars instead of billions. Instead of brute-forcing with billions in compute like OpenAI or Google, DeepSeek optimized the training process using algorithmic efficiencies. Triggered in part by U.S. sanctions that restricted access to top-tier chips, researchers in China turned constraints into opportunities for innovation. It was a reminder that technological breakthroughs can't be gatekept forever by those with the deepest pockets: when knowledge is shared, everyone benefits.

## Demystifying the Components

**A LLM** (Large Language Model) is like a master chef who has trained by tasting thousands of dishes and internalized patterns of what makes food delicious.

In reality, a model is just a giant file of numbers – millions or billions of numerical parameters encoding everything learned during training. 

**Inference** is using the model to answer questions, generate images, etc. Inference is the actual cooking process. When you ask the model to generate text, it uses those learned patterns to make decisions.

The **inference engine** is the kitchen itself: the computational environment that makes it possible for our chef to get to work.

It wasn't until I saw the price tags on the GCP Compute Engine page for GPU-resourced VMs that I began to understand just how much power LLMs need – even on a small scale!

I had been researching ratings and reviews of different models, was hearing good things about the model Mistral 7b, and was planning on using it for my app.

Then I learned that a &quot;budget&quot; or smaller version of Mistral 7b would still require ~8-12 GB of VRAM (dedicated GPU memory) to function properly. And I learned that an AWS instance that could support that could run you upwards of $300/month.

That's for a smaller version of Mistral 7b, mind you, where its inference is scaled down. The &quot;Mid-tier GPU inference&quot; – the FP16 version – would require ~24GB of VRAM for smooth inference. Which could run you upwards of $3k/month for the appropriate AWS instance.

So yes, that's why even though I definitely, totally have $3k to dish out monthly for my LLM pet project, I opted for the slowest, tiniest (but highly recommended) model I could find: GPT4All.

All of this put into perspective the computational power needed for AI on an enterprise scale. And when you think about companies training models on the entire internet's worth of data, the scale is truly mind-boggling.

## What This Project Taught Me

### 1. The Infrastructure Reality
Running even a small LLM gave me deep appreciation for the engineering challenges at scale. Meaning that optimization is no joke.

### 2. The Experimentation Framework
In an ideal world, I would have loved to deploy dozens of different models and built dashboards comparing cost, speed, accuracy, and specialization. (I know those dashboards exist somewhere and still would love to see them. 👀)

### 3. It's Time to Demystify AI
There's a lot of misunderstanding around what AI is as a technology. In order for more people to have a say in how AI shapes our world, there needs to be more readily available, accurate information about what it is and how it works. There's a need to demystify AI because it's here whether we like it or not. It's actually crucial for more people to understand how AI works because of the sheer magnitude with which it's affecting our world. It's changing our daily interactions, our work, and ultimately it's reshaping the entire economy in ways we haven't fully felt yet.

The comparison to the assembly line or steam engine isn't hyperbole. While the full consequences aren't visible yet, the transformation is already underway. Understanding how these systems work, their limitations, and how they can be changed, isn't just a question of technical curiosity, it's a collective necessity.

### 4. Creative Engineering Exists
I've always been drawn to the creative aspects of engineering: the problem-solving, the elegant solutions, the way good infrastructure feels like art. In my opinion, AI expands that human creative potential – because its power comes from human creativity and work in the first place. The question becomes – like with any epoch-shaping technology – who has oversight and calls the shots on how AI is used at the end of the day?

## The Rip Van Winkle Effect
#### (Is that a dated reference? Besides in the obvious way.)
This has been a fascinating time-skip back into tech. Missing the initial AI explosion and returning to discover a self-coding VSCode has been eye-opening to say the least.

I think AI is an incredible breakthrough with massive both creative and destructive potential. The direction it goes will be determined by who controls it in the future on a massive scale. I think the more people who learn about AI, demystify AI, and imagine ways to use it as a tool for collective good, the better off we all will be.

Understanding AI doesn't just mean understanding the math behind it or its technical functionality, but also means understanding the wide-reaching social impacts it has and will have in many aspects of life. Imagining the future of AI isn't a responsibility reserved for just MLOps engineers and researchers, this is something everyone has a stake in.

The future feels wide open, and I'm eager to keep learning – about LLMs, about infrastructure, and about the meeting point of creativity and automation. There's something energizing about being back in tech during such a transformative moment.

So even if my LLM API is so slow that it hurts, it's reminded me that understanding these systems isn't optional anymore. It's part of playing an active role in the world we're all building.

[Check out my charmingly slow API here.](https://github.com/navillasa/self-hosted-mini-llm)</content><author><name>Natalie Villasana</name></author><category term="ai" /><category term="llm" /><category term="infrastructure" /><category term="lessons" /><category term="ai" /><category term="llm" /><category term="infrastructure" /><category term="lessons" /><category term="late-adopter" /><summary type="html">I left tech to pursue advertising in summer 2022, before AI really took off. During my time as a copywriter, AI tools became widespread seemingly overnight. Taking on an LLM API project this summer was as much about refreshing my coding skills as it was about demystifying this technology that’s reshaping our world.</summary></entry><entry><title type="html">The CI/CD Pipeline I Wish I’d Built Sooner</title><link href="http://localhost:4000/blog/gitops-cicd-pipeline/" rel="alternate" type="text/html" title="The CI/CD Pipeline I Wish I’d Built Sooner" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/gitops-cicd-pipeline</id><content type="html" xml:base="http://localhost:4000/blog/gitops-cicd-pipeline/">When I started building my [TV Hub project]({% link _posts/2025-08-24-tv-hub-production-infrastructure.md %}), I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here's the GitOps pipeline I wish I'd built from day one.

## The Problem

My original setup was very simple and anxiety-inducingly fragile. Every git push triggered a full deployment to production. No gates, no testing environment, just vibes.

When a color-changing button broke the frontend one too many times, I realized I needed a system that could move fast without breaking things. The solution needed to automatically deploy tested code while preventing untested changes from reaching &quot;production.&quot;

## The Solution: Two-Speed GitOps

I ended up with a hybrid approach that treats development and production very differently:

```
git push → GitHub Actions → new images → dev deploys automatically
                                      → prod requires manual promotion
```

Here's how it works:

**Development environment:** Every successful build auto-deploys. Fast feedback, immediate testing, maximum iteration speed.

**Production environment:** Requires explicit promotion of a tested version. Slower, deliberate, with human accountability.

## How the Pipeline Works

When I push code to main, GitHub Actions runs the full test suite, builds Docker images, and tags them with a date-based version like `v20250819-f3b8541`. If tests pass, it automatically updates the development Kubernetes manifests and pushes the change back to git.

ArgoCD watches the git repo and syncs any changes to the development cluster within about 30 seconds. I can immediately test the new version at the dev URL.

For production, there's no automatic deployment. Instead, I run a promotion script:

```bash
# See what versions are available
./scripts/promote-to-prod.sh

# Promote a tested version to production  
./scripts/promote-to-prod.sh v20250819-f3b8541
```

The script shows me exactly what will change, asks for confirmation, then updates the production manifests and pushes to git. ArgoCD syncs the change to production.

## Technical Choices

I made several specific decisions that shaped how this pipeline works:

**Date-based versioning:** Instead of incrementing numbers, I use `v$(date +%Y%m%d)-$(git-hash)`. This makes it obvious when a version was created and provides a direct link back to the source code. Much easier for operations than trying to remember what v1.47 contained.

**Kustomize overlays:** The same base Kubernetes manifests work for both environments, with environment-specific patches. Development gets relaxed resource limits and debug logging; production gets proper resource constraints, and theoretically should get structured logs.

**ArgoCD split configuration:** The development ArgoCD app has `automated: true` for immediate deployments. The production app has automated sync disabled, requiring manual intervention for every change.

**Container registry strategy:** Both environments pull from the same registry, just with different tags. This ensures what I test in dev is exactly what deploys to prod—no separate build processes or subtle differences.

## What I Could Have Done Differently

**Full automation:** I could have set up comprehensive integration tests and deployed to production automatically if everything passes. The tooling exists—Argo Rollouts for progressive delivery, sophisticated test suites, automated rollback triggers.

I chose manual promotion instead because I wanted deliberate production changes. This is my portfolio project after all! The slight overhead of manual promotion forces me to actually test changes in dev and think about the impact.

**Branch-based environments:** Many teams use feature branches for feature environments, develop branch for staging, and main branch for production. I went with a simpler model where main always goes to dev, and production gets explicit promotions.

The branch approach would have been more &quot;textbook correct&quot; but would have required more operational overhead managing multiple environments and branch policies. For a portfolio project, the current approach demonstrates the core GitOps concepts without unnecessary complexity.

**External tools:** I could have used Flux instead of ArgoCD, or Tekton instead of GitHub Actions, or Jenkins X for the whole pipeline. The GitHub Actions + ArgoCD combination is incredibly common in the industry, well-documented, and has good operational tooling.

## Results

The manual promotion gate adds maybe 2 minutes to the deployment process but provides huge peace of mind. I can deploy more frequently because I'm confident each change has been tested and validated.

For a team environment, I'd probably add Slack notifications for deployments and automated integration tests against the dev environment. But for a solo project, this pipeline hits the sweet spot of safety, speed, and simplicity.

The beauty of GitOps is that these patterns scale. The same basic structure works whether you're promoting manually like I do, or using sophisticated automated promotion with comprehensive test coverage. The deployment mechanism stays consistent—only the automation boundaries change.

---

*This pipeline powers the [TV Hub project](http://tv-hub.navillasa.dev) with full source available in the [GitHub repo](https://github.com/navillasa/tv-dashboard-k8s). You can see real deployment metrics in the [live monitoring dashboard](https://monitoring.navillasa.dev/d/e0c978bd-6077-403e-ab3b-ba03f4b34962/tv-hub-business-intelligence-dashboard).*</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="gitops" /><category term="ci-cd" /><summary type="html">When I started building my TV Hub project, I did what most developers do: push to main and pray nothing breaks. Of course things did break, and I realized I needed something more sophisticated. Here’s the GitOps pipeline I wish I’d built from day one.</summary></entry><entry><title type="html">Go Big, Stay Simple: Building Production-Grade Infrastructure for a Tiny App</title><link href="http://localhost:4000/blog/tv-hub-project-overview-archived/" rel="alternate" type="text/html" title="Go Big, Stay Simple: Building Production-Grade Infrastructure for a Tiny App" /><published>2025-08-19T00:00:00-04:00</published><updated>2025-08-19T00:00:00-04:00</updated><id>http://localhost:4000/blog/tv-hub-project-overview-archived</id><content type="html" xml:base="http://localhost:4000/blog/tv-hub-project-overview-archived/">## The Paradox of Portfolio Projects

I faced a classic engineering dilemma: **build something complex that needs sophisticated infrastructure, or build something simple and add sophisticated infrastructure anyway?**

After cycling through ideas (distributed microservices, ML pipelines, IoT dashboards) I kept returning to one idea: a TV show aggregator. Because it was an app with some appeal and usefulness beyond demonstrating its own tech stack. The app itself is straightforward: fetch trending shows from TMDB and TVmaze APIs, rank by popularity, display in a clean interface.

**But there's a twist.** I deployed it with enterprise-grade DevOps practices that would typically serve a team of a 50+ engineers. Or at least a dozen.

## Strategic Design Decisions

### **Simple App, Complex Infrastructure**

I specifically chose a simple application to isolate and focus on infrastructure complexity. Here's why this approach worked:

**Cognitive Load Management**: Managing ArgoCD, Vault integration, and Prometheus monitoring is challenging enough without debugging distributed system race conditions simultaneously.

**Clear Separation of Concerns**: When something breaks, I can immediately identify whether it's an application bug (less likely, given the simplicity) or an infrastructure configuration issue (very likely, given the learning curve).

**Real-World Simulation**: Most production systems start simple and grow complex. This project demonstrates the infrastructure foundation that scales.

## Technical Architecture

The application architecture is intentionally minimal:

```
React Frontend ←→ Node.js API ←→ PostgreSQL ←→ External APIs
```

The **infrastructure architecture** tells a different story:

```
GitHub → Actions → GCR → ArgoCD → GKE Autopilot
    ↓                        ↓
Terraform → GCP Resources   Vault → External Secrets → K8s Secrets
    ↓                        ↓
Monitoring ← Prometheus ← Applications → Logs
```

### **Production-Grade Components**

**GitOps with ArgoCD**: Declarative deployments with dev/staging/prod environment promotion. Changes flow through Git, not `kubectl apply`.

**Infrastructure as Code**: Terraform manages GKE clusters, static IPs, DNS records, and IAM policies.

**Secrets Management**: HashiCorp Vault with External Secrets Operator. API keys and database credentials never touch Git or container images.

**Multi-Environment Overlays**: Kustomize overlays handle environment-specific configurations. Production can use different resource limits, replica counts, and ingress rules than development.

**Observability Stack**: Prometheus metrics collection with Grafana dashboards. Custom application metrics track API response times and cache hit rates.

## What I Learned

### **1. Mental Model Clarity**

Complex infrastructure demands clear mental models. When ArgoCD shows &quot;OutOfSync&quot; status, I need to immediately understand:
- Which Git commit should be deployed?
- What Kustomize transformations apply?
- How do external secrets affect pod startup?

**Practical Impact**: Reduced debugging time from hours to minutes by maintaining clear architectural diagrams and understanding component dependencies.

### **2. Architectural Trade-offs**

Every decision has implications. For example: [Vault as ArgoCD Application]({{ '/blog/vault-gitops/' | relative_url }}) explores why I kept secrets management outside GitOps workflows.

**Key Trade-off**: GitOps consistency vs. bootstrap complexity. Pure GitOps purists would manage Vault through ArgoCD, but circular dependencies make this operationally complex.

**Decision Framework**: Evaluate based on operational simplicity, recovery scenarios, and production patterns, not theoretical purity.

### **3. Systematic Debugging**

Production systems fail in creative ways. In [The Cluster Doctor]({{ '/blog/debugging-502-errors/' | relative_url }}) blog post I demonstrate my approach to systematic troubleshooting.

**Core Principle**: Most failures are configuration mismatches, not infrastructure failures. Port conflicts, service discovery issues, and protocol mismatches accounted for the vast majority of my deployment problems.

## Why This Approach Works

### **For Learning**
Starting with a simple application let me focus on infrastructure concepts without application complexity interference. I could experiment with GitOps workflows, monitoring configurations, and security policies without worrying about business logic bugs. For the most part :)

### **For Demonstrating**
**Systems Thinking**: Understanding how components interact across multiple layers (application, orchestration, infrastructure, security).

**Production Readiness**: Moving beyond &quot;it works on my machine&quot; to &quot;it works reliably in production with proper monitoring, security, and deployment practices.&quot;

**Technology Integration**: Connecting multiple tools (ArgoCD, Vault, Prometheus, Terraform) into cohesive workflows rather than using them in isolation.

**Decision Making**: Choosing appropriate architectural patterns based on trade-offs. A lot more fun, and difficult, than following tutorials.

## Current Status
[Live application](http://tv-hub.navillasa.dev) with full GitOps deployment, multi-environment promotion, and comprehensive monitoring.

**Next Phases**: Cost optimization dashboard, Redis caching layer, and advanced alerting rules.

## The Meta-Learning

This project demonstrates that I can build production-ready infrastructure, make informed architectural decisions, and systematically debug complex systems.

More than learning how to use individual tools, I learned how they integrate, _when_ to use them, and what trade-offs each decision creates.

Building infrastructure for a simple application taught me more about production DevOps than building a complex application with simple deployment ever could.</content><author><name>Natalie Villasana</name></author><category term="devops" /><category term="kubernetes" /><category term="architecture" /><category term="kubernetes" /><category term="argocd" /><category term="terraform" /><category term="gitops" /><category term="gke" /><category term="portfolio" /><summary type="html">The Paradox of Portfolio Projects</summary></entry><entry><title type="html">The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s</title><link href="http://localhost:4000/blog/debugging-502-errors/" rel="alternate" type="text/html" title="The Cluster Doctor, or How I Learned to Stop Worrying and Debug 502s" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/debugging-502-errors</id><content type="html" xml:base="http://localhost:4000/blog/debugging-502-errors/">## The Problem Pattern

Throughout this project, I've hit the same type of issue repeatedly:
- **Frontend** looking for `dev-backend-service` instead of `prod-backend-service`
- **ArgoCD ingress** using port 443 instead of port 80
- **Vault** connection errors due to service discovery issues
- **External Secrets** pointing to wrong Vault paths

**The common thread**: 502 Bad Gateway errors caused by communication mismatches.

## The 502 Error: What It Really Means

```
502 Bad Gateway = &quot;I can reach something, but it's not responding correctly&quot;
```

**This is different from:**
- **404**: &quot;I can't find anything at this path&quot;
- **503**: &quot;Service is temporarily unavailable&quot;  
- **Connection timeout**: &quot;I can't reach anything at all&quot;

**502 specifically means**: The proxy/load balancer reached a backend, but the backend response was invalid.

## The Systematic Debugging Approach

### Step 1: Identify the Communication Chain

Every 502 has a path like this:
```
Client → Load Balancer → Service → Pod
```

Find where it breaks by checking each link.

### Step 2: Check Backend Health in Load Balancer

```bash
# Get detailed ingress status
kubectl describe ingress &lt;ingress-name&gt; -n &lt;namespace&gt;

# Look for backend health status
# Example output:
ingress.kubernetes.io/backends: {
  &quot;k8s1-backend-service-443&quot;:&quot;UNHEALTHY&quot;  # ← The smoking gun!
}
```

**Key indicators:**
- `UNHEALTHY` = Backend isn't responding on expected port/protocol
- `HEALTHY` = Backend is fine, look elsewhere

## Pro Tips for Faster Debugging

### 1. **Use Port-Forward for Direct Testing**
```bash
# Bypass ingress/service and test pod directly
kubectl port-forward pod/&lt;pod-name&gt; 8080:8080
curl http://localhost:8080/health
```

### 2. **Check Multiple Namespaces**
```bash
# Service in wrong namespace is super common
kubectl get services --all-namespaces | grep &lt;service-name&gt;
```

The key insight: Most 502 errors are configuration mismatches, not infrastructure failures. A systematic approach beats random troubleshooting every time.</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="debugging" /><category term="devops" /><category term="kubernetes" /><category term="502-errors" /><category term="troubleshooting" /><category term="nginx" /><category term="ingress" /><summary type="html">The Problem Pattern</summary></entry><entry><title type="html">Why I Didn’t Make Vault an ArgoCD Application</title><link href="http://localhost:4000/blog/vault-gitops/" rel="alternate" type="text/html" title="Why I Didn’t Make Vault an ArgoCD Application" /><published>2025-08-18T00:00:00-04:00</published><updated>2025-08-18T00:00:00-04:00</updated><id>http://localhost:4000/blog/vault-gitops</id><content type="html" xml:base="http://localhost:4000/blog/vault-gitops/">## The Question

While building my [TV Dashboard on Kubernetes project]({{ '/blog/go-big-stay-simple/' | relative_url }}) – a production-grade streaming analytics platform running on GKE – I faced an interesting architectural decision during the GitOps implementation. Should HashiCorp Vault be managed as an ArgoCD application alongside the other workloads? This post explains why I chose to keep Vault outside of the GitOps workflow.

## The Temptation

At first glance, managing Vault through ArgoCD seems _pretty_ cool:

- **Consistency**: Everything else is managed by ArgoCD
- **Version Control**: Vault configuration tracked in Git
- **Declarative**: Infrastructure as Code principles
- **Automation**: Automated deployments and updates

## The Bootstrap Problem

The fundamental issue is a **circular dependency**:

```
┌─────────────┐    needs    ┌──────────────────┐    needs    ┌───────────┐
│   ArgoCD    │ ──────────► │ External Secrets │ ──────────► │   Vault   │
│             │             │    Operator      │             │           │
└─────────────┘             └──────────────────┘             └───────────┘
       ▲                                                            │
       │                    manages (if GitOps)                     │
       └────────────────────────────────────────────────────────────┘
```

**The problem**: If ArgoCD manages Vault, then ArgoCD depends on Vault (for secrets) which depends on ArgoCD (for deployment). This creates an unresolvable bootstrap dependency.

## Infrastructure Layers

I solved this by establishing clear **infrastructure layers**:

### Layer 0: Foundation Infrastructure
- **GKE Cluster** (Terraform)
- **Static IPs** (Terraform)
- **Vault** (kubectl apply)
- **External Secrets Operator** (Helm)

### Layer 1: GitOps Platform
- **ArgoCD** (kubectl apply)
- **ClusterSecretStore** (ArgoCD)

### Layer 2: Applications
- **tv-dashboard-dev** (ArgoCD)
- **tv-dashboard-prod** (ArgoCD)
- **monitoring-stack** (ArgoCD)

## Alternative Approaches Considered

### 1. Manual Sync Only
```yaml
# vault-app.yaml
spec:
  syncPolicy:
    automated: null  # No auto-sync to avoid bootstrap issues
```

**Pros**: Vault config in Git  
**Cons**: Manual intervention required, defeats GitOps automation

### 2. App-of-Apps Pattern
```yaml
# bootstrap-app.yaml - deployed manually
# Manages vault-app.yaml and argocd-apps.yaml
```

**Pros**: Complete GitOps coverage  
**Cons**: Complex bootstrap sequence, harder to debug

### 3. External Vault
Use a managed secret service (Google Secret Manager, AWS Secrets Manager)

**Pros**: No bootstrap problem, less operational overhead  
**Cons**: Vendor lock-in, less learning value for this project

## The Decision: Keep Vault Outside GitOps

**Reasons**:

1. **Operational Simplicity**: Vault is foundational infrastructure that should be stable and simple to manage
2. **Recovery Scenarios**: If ArgoCD fails, we can still access Vault to recover secrets
3. **Bootstrap Clarity**: Clear separation between foundation and application layers
4. **Production Patterns**: Many organizations treat secret management as Layer 0 infrastructure

## Production Considerations

In production environments, you might see:

### At Large Organizations
- **Dedicated Vault clusters** managed by platform teams
- **Vault-as-a-Service** provided to application teams
- **Manual deployment** with infrastructure automation (Terraform)

### At Cloud-Native Shops
- **Managed secret services** (AWS Secrets Manager, etc.)
- **External Secrets Operator** connecting to cloud providers
- **No self-hosted Vault** at all

### In GitOps Purist Repos
- **Everything in Git** including Vault
- **Complex bootstrap procedures** with operator dependencies
- **Acceptance of operational complexity** for consistency

## Key Takeaway

**Not everything needs to be in GitOps.** The right architectural boundary depends on:

- **Operational complexity** vs **consistency benefits**
- **Bootstrap dependencies** and **recovery scenarios**  
- **Team structure** and **operational expertise**
- **Compliance requirements** and **audit trails**

For my TV Dashboard project, keeping Vault as foundational infrastructure provided the right balance of simplicity and functionality while still demonstrating modern secret management practices.

## What I Learned

1. **Architectural boundaries matter** - not every tool fits every pattern
2. **Bootstrap dependencies** are real constraints in system design
3. **Operational simplicity** often trumps theoretical consistency
4. **Production patterns** should inform learning project decisions

The goal isn't to GitOps-ify everything — it's to build systems that are **reliable, maintainable, and appropriate for their context**.

---

*This decision reflects my specific context and learning goals. Your environment may have different constraints and requirements.*</content><author><name>Natalie Villasana</name></author><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="kubernetes" /><category term="vault" /><category term="gitops" /><category term="argocd" /><category term="devops" /><category term="design" /><summary type="html">The Question</summary></entry></feed>